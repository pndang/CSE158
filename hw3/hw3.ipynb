{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f55c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005c1a02-c5bf-4241-8d00-dc260d36f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54fa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    f = gzip.open(path, 'rt', encoding='utf8')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        g = d['gameID']\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c215087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27aec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful\n",
    "\n",
    "gamesPerUser = defaultdict(set)\n",
    "allGames = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72d24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"data/train.json.gz\"):\n",
    "    allHours.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462bd9bd-b657-4571-8fef-eee11d852099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58fd9e5-0ba6-4fef-83c1-315503d75348",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Play prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e06cc33-bc60-4b45-be63-8033c17d9fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u35844632',\n",
       " 'g92018877',\n",
       " {'hours': 1.2,\n",
       "  'gameID': 'g92018877',\n",
       "  'hours_transformed': 1.1375035237499351,\n",
       "  'early_access': False,\n",
       "  'date': '2015-11-22',\n",
       "  'text': \"I didn't understand a single thing, but it was a fun game!\",\n",
       "  'userID': 'u35844632'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any other preprocessing...\n",
    "\n",
    "hoursValid[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f46fd0-3abb-4f46-8a8b-9cf37efa99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "\n",
    "for h in allHours:\n",
    "    uid = h[0]\n",
    "    gid = h[1]\n",
    "    gamesPerUser[uid].add(gid)\n",
    "    allGames.add(gid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling\n",
    "\n",
    "newValid = []\n",
    "for h in hoursValid:\n",
    "    uid = h[0]\n",
    "    haveNotPlayed = []\n",
    "    for g in allGames:\n",
    "        if g not in gamesPerUser[uid]:\n",
    "            haveNotPlayed.append(g)\n",
    "    newValid.append((uid, h[1]))\n",
    "    newValid.append((uid, random.choice(haveNotPlayed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa30a66-0dcd-4f1f-beb0-a6ba473c9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline strategy\n",
    "\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user,game,_ in readJSON(\"data/train.json.gz\"):\n",
    "  gameCount[game] += 1\n",
    "  totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += ic\n",
    "  return1.add(i)\n",
    "  if count > totalPlayed/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u00914251', 'g61913894'),\n",
       " ('u00914251', 'g29034331'),\n",
       " ('u44824365', 'g16586637'),\n",
       " ('u44824365', 'g32003820'),\n",
       " ('u69168994', 'g51450658'),\n",
       " ('u69168994', 'g98961542'),\n",
       " ('u74934282', 'g45232069'),\n",
       " ('u74934282', 'g21020026'),\n",
       " ('u02906788', 'g36958384'),\n",
       " ('u02906788', 'g08978402'),\n",
       " ('u35844632', 'g92018877'),\n",
       " ('u35844632', 'g46602286'),\n",
       " ('u08586826', 'g36769937'),\n",
       " ('u08586826', 'g45024343'),\n",
       " ('u92465525', 'g01549519'),\n",
       " ('u92465525', 'g21091447'),\n",
       " ('u50690284', 'g05349653'),\n",
       " ('u50690284', 'g99934462'),\n",
       " ('u34690083', 'g08001721'),\n",
       " ('u34690083', 'g43188612'),\n",
       " ('u79438356', 'g04477361'),\n",
       " ('u79438356', 'g90153689'),\n",
       " ('u84107660', 'g11225866'),\n",
       " ('u84107660', 'g80627411'),\n",
       " ('u24256911', 'g67274965'),\n",
       " ('u24256911', 'g76099968'),\n",
       " ('u23937279', 'g31190276'),\n",
       " ('u23937279', 'g57658162'),\n",
       " ('u15844193', 'g55567630'),\n",
       " ('u15844193', 'g90348447'),\n",
       " ('u66008466', 'g12667678'),\n",
       " ('u66008466', 'g87968210'),\n",
       " ('u79801558', 'g27370047'),\n",
       " ('u79801558', 'g19831304'),\n",
       " ('u42207476', 'g26437139'),\n",
       " ('u42207476', 'g90398212'),\n",
       " ('u81077786', 'g91799982'),\n",
       " ('u81077786', 'g62087868'),\n",
       " ('u08258862', 'g59317800'),\n",
       " ('u08258862', 'g99554488'),\n",
       " ('u77378317', 'g87166472'),\n",
       " ('u77378317', 'g89793168'),\n",
       " ('u51062008', 'g19425343'),\n",
       " ('u51062008', 'g72873062'),\n",
       " ('u79025668', 'g05031080'),\n",
       " ('u79025668', 'g46013492'),\n",
       " ('u89703849', 'g73173393'),\n",
       " ('u89703849', 'g08353646'),\n",
       " ('u55093422', 'g54979645'),\n",
       " ('u55093422', 'g13343607'),\n",
       " ('u29576207', 'g57869201'),\n",
       " ('u29576207', 'g32819803'),\n",
       " ('u05088496', 'g00125299'),\n",
       " ('u05088496', 'g57588908'),\n",
       " ('u89482132', 'g59638098'),\n",
       " ('u89482132', 'g00512355'),\n",
       " ('u39599062', 'g75228197'),\n",
       " ('u39599062', 'g09698387'),\n",
       " ('u83720525', 'g31948580'),\n",
       " ('u83720525', 'g10183897'),\n",
       " ('u68722473', 'g75513928'),\n",
       " ('u68722473', 'g28952305'),\n",
       " ('u52693207', 'g99934462'),\n",
       " ('u52693207', 'g67221114'),\n",
       " ('u69957279', 'g51770536'),\n",
       " ('u69957279', 'g46448875'),\n",
       " ('u76695873', 'g49636917'),\n",
       " ('u76695873', 'g53908452'),\n",
       " ('u22769746', 'g22927590'),\n",
       " ('u22769746', 'g75046549'),\n",
       " ('u73312807', 'g17074600'),\n",
       " ('u73312807', 'g31800990'),\n",
       " ('u44594988', 'g91909847'),\n",
       " ('u44594988', 'g71764063'),\n",
       " ('u96980242', 'g83146398'),\n",
       " ('u96980242', 'g23906482'),\n",
       " ('u25042283', 'g08346452'),\n",
       " ('u25042283', 'g69213131'),\n",
       " ('u85288110', 'g54619657'),\n",
       " ('u85288110', 'g05227907'),\n",
       " ('u69394986', 'g59682009'),\n",
       " ('u69394986', 'g76846965'),\n",
       " ('u66399737', 'g73594265'),\n",
       " ('u66399737', 'g43409822'),\n",
       " ('u08942444', 'g84770347'),\n",
       " ('u08942444', 'g01679750'),\n",
       " ('u26800137', 'g68547272'),\n",
       " ('u26800137', 'g22771657'),\n",
       " ('u29718959', 'g76960634'),\n",
       " ('u29718959', 'g21883457'),\n",
       " ('u62402243', 'g28846994'),\n",
       " ('u62402243', 'g44582104'),\n",
       " ('u46285135', 'g78336897'),\n",
       " ('u46285135', 'g56626398'),\n",
       " ('u90784003', 'g90898833'),\n",
       " ('u90784003', 'g18542484'),\n",
       " ('u84685369', 'g22161219'),\n",
       " ('u84685369', 'g96482153'),\n",
       " ('u11194965', 'g58464197'),\n",
       " ('u11194965', 'g81135807'),\n",
       " ('u61089862', 'g72460998'),\n",
       " ('u61089862', 'g74493125'),\n",
       " ('u33951621', 'g77806076'),\n",
       " ('u33951621', 'g68940279'),\n",
       " ('u23335732', 'g03167855'),\n",
       " ('u23335732', 'g56536263'),\n",
       " ('u99613619', 'g10946181'),\n",
       " ('u99613619', 'g59848866'),\n",
       " ('u98484614', 'g66296428'),\n",
       " ('u98484614', 'g10675976'),\n",
       " ('u21261196', 'g31800990'),\n",
       " ('u21261196', 'g13451594'),\n",
       " ('u35298149', 'g61225634'),\n",
       " ('u35298149', 'g86428226'),\n",
       " ('u56399548', 'g35296108'),\n",
       " ('u56399548', 'g97873808'),\n",
       " ('u59370809', 'g31712310'),\n",
       " ('u59370809', 'g86265809'),\n",
       " ('u55775774', 'g05914065'),\n",
       " ('u55775774', 'g88090167'),\n",
       " ('u26651259', 'g36222633'),\n",
       " ('u26651259', 'g27594854'),\n",
       " ('u23210251', 'g85900991'),\n",
       " ('u23210251', 'g12946119'),\n",
       " ('u17224257', 'g85094839'),\n",
       " ('u17224257', 'g48254339'),\n",
       " ('u49076616', 'g48657523'),\n",
       " ('u49076616', 'g68302537'),\n",
       " ('u53420470', 'g04830062'),\n",
       " ('u53420470', 'g06975986'),\n",
       " ('u16116202', 'g61958781'),\n",
       " ('u16116202', 'g42574843'),\n",
       " ('u58246803', 'g79499736'),\n",
       " ('u58246803', 'g87749248'),\n",
       " ('u50987399', 'g82616432'),\n",
       " ('u50987399', 'g26522881'),\n",
       " ('u67143531', 'g14525638'),\n",
       " ('u67143531', 'g47981354'),\n",
       " ('u93655272', 'g76120150'),\n",
       " ('u93655272', 'g05181387'),\n",
       " ('u25438529', 'g47205010'),\n",
       " ('u25438529', 'g73594265'),\n",
       " ('u97999106', 'g61264440'),\n",
       " ('u97999106', 'g61214579'),\n",
       " ('u98443157', 'g61214579'),\n",
       " ('u98443157', 'g29077114'),\n",
       " ('u54507745', 'g41331921'),\n",
       " ('u54507745', 'g75042518'),\n",
       " ('u51175241', 'g90154665'),\n",
       " ('u51175241', 'g10932727'),\n",
       " ('u35666144', 'g94005519'),\n",
       " ('u35666144', 'g82621570'),\n",
       " ('u52585979', 'g38434648'),\n",
       " ('u52585979', 'g92237283'),\n",
       " ('u15827297', 'g69033010'),\n",
       " ('u15827297', 'g33466586'),\n",
       " ('u80016522', 'g41348232'),\n",
       " ('u80016522', 'g24816539'),\n",
       " ('u61755488', 'g59271404'),\n",
       " ('u61755488', 'g10962300'),\n",
       " ('u65526955', 'g35972218'),\n",
       " ('u65526955', 'g38057988'),\n",
       " ('u79311898', 'g39055346'),\n",
       " ('u79311898', 'g42154675'),\n",
       " ('u53316375', 'g48657523'),\n",
       " ('u53316375', 'g29578492'),\n",
       " ('u61948708', 'g48512616'),\n",
       " ('u61948708', 'g92405872'),\n",
       " ('u76337229', 'g24822320'),\n",
       " ('u76337229', 'g05342501'),\n",
       " ('u42896872', 'g93472243'),\n",
       " ('u42896872', 'g84127019'),\n",
       " ('u85921592', 'g50163420'),\n",
       " ('u85921592', 'g47086763'),\n",
       " ('u20132431', 'g12460730'),\n",
       " ('u20132431', 'g84020059'),\n",
       " ('u00715488', 'g28303713'),\n",
       " ('u00715488', 'g54465548'),\n",
       " ('u60564624', 'g70466974'),\n",
       " ('u60564624', 'g71624193'),\n",
       " ('u62593459', 'g88216046'),\n",
       " ('u62593459', 'g68132896'),\n",
       " ('u30647811', 'g46446145'),\n",
       " ('u30647811', 'g19475656'),\n",
       " ('u98158963', 'g24479985'),\n",
       " ('u98158963', 'g14368997'),\n",
       " ('u13620759', 'g84207332'),\n",
       " ('u13620759', 'g63074330'),\n",
       " ('u46422197', 'g93941352'),\n",
       " ('u46422197', 'g01959969'),\n",
       " ('u53543007', 'g77400036'),\n",
       " ('u53543007', 'g46932837'),\n",
       " ('u96364924', 'g29752784'),\n",
       " ('u96364924', 'g32683225'),\n",
       " ('u57027153', 'g46950698'),\n",
       " ('u57027153', 'g46101992'),\n",
       " ('u81310029', 'g64574718'),\n",
       " ('u81310029', 'g44025476'),\n",
       " ('u70835980', 'g17807252'),\n",
       " ('u70835980', 'g04226156'),\n",
       " ('u60584353', 'g85103579'),\n",
       " ('u60584353', 'g92740322'),\n",
       " ('u57130814', 'g42907287'),\n",
       " ('u57130814', 'g72574918'),\n",
       " ('u18198302', 'g79333823'),\n",
       " ('u18198302', 'g62014021'),\n",
       " ('u29004052', 'g86787099'),\n",
       " ('u29004052', 'g02637258'),\n",
       " ('u35434324', 'g05339526'),\n",
       " ('u35434324', 'g56247984'),\n",
       " ('u29419712', 'g67139666'),\n",
       " ('u29419712', 'g34412552'),\n",
       " ('u25657821', 'g42403034'),\n",
       " ('u25657821', 'g32579898'),\n",
       " ('u14083278', 'g15217706'),\n",
       " ('u14083278', 'g46979275'),\n",
       " ('u84062881', 'g22839997'),\n",
       " ('u84062881', 'g16340026'),\n",
       " ('u15763015', 'g85026274'),\n",
       " ('u15763015', 'g52140787'),\n",
       " ('u81779282', 'g35449408'),\n",
       " ('u81779282', 'g53460949'),\n",
       " ('u67081608', 'g03454293'),\n",
       " ('u67081608', 'g19759241'),\n",
       " ('u46422197', 'g11862712'),\n",
       " ('u46422197', 'g42986038'),\n",
       " ('u80165894', 'g23403411'),\n",
       " ('u80165894', 'g25612121'),\n",
       " ('u20778676', 'g34765721'),\n",
       " ('u20778676', 'g96340072'),\n",
       " ('u31052108', 'g23885183'),\n",
       " ('u31052108', 'g75521004'),\n",
       " ('u67084531', 'g49208981'),\n",
       " ('u67084531', 'g76717273'),\n",
       " ('u50097048', 'g10406824'),\n",
       " ('u50097048', 'g17514824'),\n",
       " ('u60956035', 'g62805978'),\n",
       " ('u60956035', 'g31756494'),\n",
       " ('u54669683', 'g29326880'),\n",
       " ('u54669683', 'g60046113'),\n",
       " ('u11593267', 'g54087643'),\n",
       " ('u11593267', 'g00795234'),\n",
       " ('u64398351', 'g79866169'),\n",
       " ('u64398351', 'g31270106'),\n",
       " ('u89478667', 'g92454558'),\n",
       " ('u89478667', 'g02948692'),\n",
       " ('u94977887', 'g73265847'),\n",
       " ('u94977887', 'g66216718'),\n",
       " ('u66968893', 'g89809648'),\n",
       " ('u66968893', 'g06199403'),\n",
       " ('u68225433', 'g52414449'),\n",
       " ('u68225433', 'g36859945'),\n",
       " ('u06535577', 'g48350599'),\n",
       " ('u06535577', 'g96839895'),\n",
       " ('u36025454', 'g15536656'),\n",
       " ('u36025454', 'g49488346'),\n",
       " ('u96019728', 'g06097413'),\n",
       " ('u96019728', 'g62161670'),\n",
       " ('u60473832', 'g02141122'),\n",
       " ('u60473832', 'g03454293'),\n",
       " ('u42702163', 'g83093985'),\n",
       " ('u42702163', 'g96307947'),\n",
       " ('u15111837', 'g22386746'),\n",
       " ('u15111837', 'g46421079'),\n",
       " ('u29419712', 'g40372626'),\n",
       " ('u29419712', 'g32095755'),\n",
       " ('u02906788', 'g03136155'),\n",
       " ('u02906788', 'g20805250'),\n",
       " ('u49817851', 'g35331676'),\n",
       " ('u49817851', 'g69336814'),\n",
       " ('u93087874', 'g16952539'),\n",
       " ('u93087874', 'g30105066'),\n",
       " ('u42921970', 'g81608348'),\n",
       " ('u42921970', 'g90827572'),\n",
       " ('u62610693', 'g22161219'),\n",
       " ('u62610693', 'g94730688'),\n",
       " ('u86837249', 'g33872505'),\n",
       " ('u86837249', 'g96839895'),\n",
       " ('u91640743', 'g06097413'),\n",
       " ('u91640743', 'g36281648'),\n",
       " ('u61082324', 'g16952539'),\n",
       " ('u61082324', 'g59544573'),\n",
       " ('u82502934', 'g78970626'),\n",
       " ('u82502934', 'g87889911'),\n",
       " ('u66004745', 'g52187292'),\n",
       " ('u66004745', 'g38792981'),\n",
       " ('u10402813', 'g02637258'),\n",
       " ('u10402813', 'g30639399'),\n",
       " ('u08730023', 'g91024164'),\n",
       " ('u08730023', 'g15632833'),\n",
       " ('u31609337', 'g10406824'),\n",
       " ('u31609337', 'g43300921'),\n",
       " ('u92134089', 'g32886144'),\n",
       " ('u92134089', 'g35073657'),\n",
       " ('u76745785', 'g43692578'),\n",
       " ('u76745785', 'g31865043'),\n",
       " ('u10804532', 'g59544573'),\n",
       " ('u10804532', 'g69564979'),\n",
       " ('u11464339', 'g06045292'),\n",
       " ('u11464339', 'g19972407'),\n",
       " ('u84554687', 'g28740995'),\n",
       " ('u84554687', 'g51181091'),\n",
       " ('u58947884', 'g02428379'),\n",
       " ('u58947884', 'g16340026'),\n",
       " ('u93915033', 'g38617825'),\n",
       " ('u93915033', 'g40531835'),\n",
       " ('u85183634', 'g57734956'),\n",
       " ('u85183634', 'g33400185'),\n",
       " ('u37959553', 'g66220144'),\n",
       " ('u37959553', 'g57932017'),\n",
       " ('u52695457', 'g89492775'),\n",
       " ('u52695457', 'g55998281'),\n",
       " ('u85588071', 'g10259243'),\n",
       " ('u85588071', 'g25775891'),\n",
       " ('u78578337', 'g45519076'),\n",
       " ('u78578337', 'g28405264'),\n",
       " ('u75597994', 'g79333823'),\n",
       " ('u75597994', 'g10399772'),\n",
       " ('u99590760', 'g05349653'),\n",
       " ('u99590760', 'g52261283'),\n",
       " ('u38718529', 'g18283520'),\n",
       " ('u38718529', 'g69744925'),\n",
       " ('u95213459', 'g84254492'),\n",
       " ('u95213459', 'g42124312'),\n",
       " ('u26651259', 'g02903254'),\n",
       " ('u26651259', 'g52947714'),\n",
       " ('u72952508', 'g67293287'),\n",
       " ('u72952508', 'g72574918'),\n",
       " ('u37274930', 'g11919936'),\n",
       " ('u37274930', 'g22244120'),\n",
       " ('u82276354', 'g18658878'),\n",
       " ('u82276354', 'g54894888'),\n",
       " ('u92083302', 'g24982722'),\n",
       " ('u92083302', 'g72460998'),\n",
       " ('u27823790', 'g49593949'),\n",
       " ('u27823790', 'g48289742'),\n",
       " ('u16220374', 'g25077590'),\n",
       " ('u16220374', 'g18529610'),\n",
       " ('u39534714', 'g91799982'),\n",
       " ('u39534714', 'g78674402'),\n",
       " ('u36252269', 'g67366154'),\n",
       " ('u36252269', 'g97795987'),\n",
       " ('u87799097', 'g61225634'),\n",
       " ('u87799097', 'g96079291'),\n",
       " ('u84512016', 'g76810548'),\n",
       " ('u84512016', 'g93325439'),\n",
       " ('u61150089', 'g98217589'),\n",
       " ('u61150089', 'g58375086'),\n",
       " ('u97099051', 'g02230168'),\n",
       " ('u97099051', 'g83737994'),\n",
       " ('u62365327', 'g94132851'),\n",
       " ('u62365327', 'g63396588'),\n",
       " ('u31135608', 'g58129520'),\n",
       " ('u31135608', 'g70822619'),\n",
       " ('u69137549', 'g47086763'),\n",
       " ('u69137549', 'g39176429'),\n",
       " ('u11400327', 'g82994209'),\n",
       " ('u11400327', 'g23825288'),\n",
       " ('u90945829', 'g85252310'),\n",
       " ('u90945829', 'g70793668'),\n",
       " ('u92729375', 'g03706634'),\n",
       " ('u92729375', 'g29298348'),\n",
       " ('u54111399', 'g37198629'),\n",
       " ('u54111399', 'g66069533'),\n",
       " ('u68530602', 'g06324638'),\n",
       " ('u68530602', 'g05551150'),\n",
       " ('u25180573', 'g39366834'),\n",
       " ('u25180573', 'g42406963'),\n",
       " ('u74426406', 'g83757895'),\n",
       " ('u74426406', 'g86195002'),\n",
       " ('u67309742', 'g51283481'),\n",
       " ('u67309742', 'g91879395'),\n",
       " ('u64030091', 'g40499587'),\n",
       " ('u64030091', 'g44130855'),\n",
       " ('u24541889', 'g47446961'),\n",
       " ('u24541889', 'g70463993'),\n",
       " ('u20316991', 'g66324364'),\n",
       " ('u20316991', 'g09558961'),\n",
       " ('u69861266', 'g56626398'),\n",
       " ('u69861266', 'g63149989'),\n",
       " ('u14407800', 'g25077590'),\n",
       " ('u14407800', 'g94403031'),\n",
       " ('u34600363', 'g74210041'),\n",
       " ('u34600363', 'g32886144'),\n",
       " ('u31605448', 'g47290851'),\n",
       " ('u31605448', 'g07804237'),\n",
       " ('u06266052', 'g64348065'),\n",
       " ('u06266052', 'g21101440'),\n",
       " ('u02402833', 'g54465548'),\n",
       " ('u02402833', 'g62054141'),\n",
       " ('u52498232', 'g72683624'),\n",
       " ('u52498232', 'g23815030'),\n",
       " ('u42411555', 'g02637258'),\n",
       " ('u42411555', 'g79449190'),\n",
       " ('u33908008', 'g09670581'),\n",
       " ('u33908008', 'g05349653'),\n",
       " ('u69825040', 'g46446145'),\n",
       " ('u69825040', 'g01392148'),\n",
       " ('u31548470', 'g49387258'),\n",
       " ('u31548470', 'g53635516'),\n",
       " ('u64157292', 'g16653801'),\n",
       " ('u64157292', 'g55239305'),\n",
       " ('u36610257', 'g41896919'),\n",
       " ('u36610257', 'g04880803'),\n",
       " ('u75552947', 'g33598656'),\n",
       " ('u75552947', 'g68547272'),\n",
       " ('u48340663', 'g79333823'),\n",
       " ('u48340663', 'g94053053'),\n",
       " ('u69465618', 'g36958384'),\n",
       " ('u69465618', 'g78239118'),\n",
       " ('u90130076', 'g37899466'),\n",
       " ('u90130076', 'g38302836'),\n",
       " ('u45711549', 'g42862258'),\n",
       " ('u45711549', 'g33263421'),\n",
       " ('u32456321', 'g99674775'),\n",
       " ('u32456321', 'g68182978'),\n",
       " ('u04695380', 'g26730287'),\n",
       " ('u04695380', 'g45244601'),\n",
       " ('u31245260', 'g80441672'),\n",
       " ('u31245260', 'g67038900'),\n",
       " ('u74573586', 'g12911763'),\n",
       " ('u74573586', 'g89122297'),\n",
       " ('u36078563', 'g75160975'),\n",
       " ('u36078563', 'g29657740'),\n",
       " ('u03887694', 'g82275622'),\n",
       " ('u03887694', 'g51450658'),\n",
       " ('u06109427', 'g67038900'),\n",
       " ('u06109427', 'g12911763'),\n",
       " ('u60975696', 'g29494592'),\n",
       " ('u60975696', 'g26569451'),\n",
       " ('u33085406', 'g10876271'),\n",
       " ('u33085406', 'g99500667'),\n",
       " ('u37098825', 'g06378804'),\n",
       " ('u37098825', 'g81456217'),\n",
       " ('u14981323', 'g22868280'),\n",
       " ('u14981323', 'g65756682'),\n",
       " ('u08135758', 'g24809309'),\n",
       " ('u08135758', 'g43321406'),\n",
       " ('u22096553', 'g00663854'),\n",
       " ('u22096553', 'g83532643'),\n",
       " ('u46685808', 'g76745472'),\n",
       " ('u46685808', 'g85212990'),\n",
       " ('u66660782', 'g16087475'),\n",
       " ('u66660782', 'g88090167'),\n",
       " ('u58998893', 'g54703242'),\n",
       " ('u58998893', 'g26965656'),\n",
       " ('u33994300', 'g41623816'),\n",
       " ('u33994300', 'g84686560'),\n",
       " ('u17759890', 'g34412552'),\n",
       " ('u17759890', 'g76602932'),\n",
       " ('u55309834', 'g53908452'),\n",
       " ('u55309834', 'g66575805'),\n",
       " ('u78532330', 'g68302537'),\n",
       " ('u78532330', 'g57187406'),\n",
       " ('u45713244', 'g03136155'),\n",
       " ('u45713244', 'g84720805'),\n",
       " ('u25036365', 'g89793168'),\n",
       " ('u25036365', 'g81175459'),\n",
       " ('u09833175', 'g10879560'),\n",
       " ('u09833175', 'g47352460'),\n",
       " ('u59568450', 'g15870705'),\n",
       " ('u59568450', 'g60509936'),\n",
       " ('u72460514', 'g46287090'),\n",
       " ('u72460514', 'g81135807'),\n",
       " ('u21233168', 'g47087082'),\n",
       " ('u21233168', 'g93034225'),\n",
       " ('u02507193', 'g73265847'),\n",
       " ('u02507193', 'g63466468'),\n",
       " ('u42025067', 'g19475656'),\n",
       " ('u42025067', 'g08352025'),\n",
       " ('u03516535', 'g27370047'),\n",
       " ('u03516535', 'g15870705'),\n",
       " ('u89855328', 'g70274773'),\n",
       " ('u89855328', 'g65175011'),\n",
       " ('u71346354', 'g01914885'),\n",
       " ('u71346354', 'g53024880'),\n",
       " ('u92905321', 'g04025903'),\n",
       " ('u92905321', 'g55335836'),\n",
       " ('u19719654', 'g15881340'),\n",
       " ('u19719654', 'g85117340'),\n",
       " ('u58912859', 'g37446712'),\n",
       " ('u58912859', 'g45259451'),\n",
       " ('u29017568', 'g36558912'),\n",
       " ('u29017568', 'g29170259'),\n",
       " ('u47470615', 'g34477578'),\n",
       " ('u47470615', 'g79563349'),\n",
       " ('u79574037', 'g53223749'),\n",
       " ('u79574037', 'g29808295'),\n",
       " ('u54479503', 'g37422652'),\n",
       " ('u54479503', 'g69356603'),\n",
       " ('u50086910', 'g28303713'),\n",
       " ('u50086910', 'g66712584'),\n",
       " ('u64368070', 'g95116279'),\n",
       " ('u64368070', 'g47290851'),\n",
       " ('u02276429', 'g49368897'),\n",
       " ('u02276429', 'g38649631'),\n",
       " ('u03203266', 'g51367911'),\n",
       " ('u03203266', 'g63870321'),\n",
       " ('u15250211', 'g21037002'),\n",
       " ('u15250211', 'g84994417'),\n",
       " ('u82528197', 'g07804237'),\n",
       " ('u82528197', 'g56460004'),\n",
       " ('u90509755', 'g36122744'),\n",
       " ('u90509755', 'g33108407'),\n",
       " ('u29302274', 'g83424757'),\n",
       " ('u29302274', 'g32989876'),\n",
       " ('u95484217', 'g96083451'),\n",
       " ('u95484217', 'g27495791'),\n",
       " ('u49842960', 'g21037002'),\n",
       " ('u49842960', 'g07535543'),\n",
       " ('u41227532', 'g59109370'),\n",
       " ('u41227532', 'g14120436'),\n",
       " ('u32771256', 'g10932727'),\n",
       " ('u32771256', 'g03136155'),\n",
       " ('u61791810', 'g40551467'),\n",
       " ('u61791810', 'g20051407'),\n",
       " ('u19057918', 'g41896919'),\n",
       " ('u19057918', 'g15360200'),\n",
       " ('u02126281', 'g62850718'),\n",
       " ('u02126281', 'g71822589'),\n",
       " ('u32158093', 'g82317851'),\n",
       " ('u32158093', 'g99613754'),\n",
       " ('u21934617', 'g32028748'),\n",
       " ('u21934617', 'g39179145'),\n",
       " ('u32473559', 'g10773791'),\n",
       " ('u32473559', 'g01030828'),\n",
       " ('u66004745', 'g65734768'),\n",
       " ('u66004745', 'g57032458'),\n",
       " ('u99921366', 'g75632390'),\n",
       " ('u99921366', 'g35914138'),\n",
       " ('u18846988', 'g83595955'),\n",
       " ('u18846988', 'g34381901'),\n",
       " ('u18406102', 'g61465953'),\n",
       " ('u18406102', 'g17907308'),\n",
       " ('u91673094', 'g83853612'),\n",
       " ('u91673094', 'g24849398'),\n",
       " ('u37103371', 'g56396594'),\n",
       " ('u37103371', 'g52937196'),\n",
       " ('u30258510', 'g68302537'),\n",
       " ('u30258510', 'g06266910'),\n",
       " ('u19599296', 'g52947714'),\n",
       " ('u19599296', 'g82654445'),\n",
       " ('u97799002', 'g52620301'),\n",
       " ('u97799002', 'g62180900'),\n",
       " ('u73074491', 'g54619657'),\n",
       " ('u73074491', 'g00453873'),\n",
       " ('u88361580', 'g68135850'),\n",
       " ('u88361580', 'g47321452'),\n",
       " ('u21233168', 'g29741733'),\n",
       " ('u21233168', 'g44582104'),\n",
       " ('u08898885', 'g27545941'),\n",
       " ('u08898885', 'g72856076'),\n",
       " ('u24898713', 'g10444367'),\n",
       " ('u24898713', 'g84254492'),\n",
       " ('u64337644', 'g26556480'),\n",
       " ('u64337644', 'g66192174'),\n",
       " ('u79303552', 'g03128638'),\n",
       " ('u79303552', 'g69372699'),\n",
       " ('u05692952', 'g65773934'),\n",
       " ('u05692952', 'g47957144'),\n",
       " ('u91343803', 'g81456217'),\n",
       " ('u91343803', 'g53078416'),\n",
       " ('u04836696', 'g41141819'),\n",
       " ('u04836696', 'g36471550'),\n",
       " ('u89365224', 'g36122744'),\n",
       " ('u89365224', 'g84276329'),\n",
       " ('u20237145', 'g18542484'),\n",
       " ('u20237145', 'g12540365'),\n",
       " ('u93124412', 'g34193208'),\n",
       " ('u93124412', 'g82220080'),\n",
       " ('u25150038', 'g61076572'),\n",
       " ('u25150038', 'g76381409'),\n",
       " ('u67243762', 'g04830062'),\n",
       " ('u67243762', 'g03200916'),\n",
       " ('u97764793', 'g05974045'),\n",
       " ('u97764793', 'g77288617'),\n",
       " ('u42166729', 'g60416652'),\n",
       " ('u42166729', 'g72028177'),\n",
       " ('u62757983', 'g32799642'),\n",
       " ('u62757983', 'g39090767'),\n",
       " ('u57035589', 'g53715495'),\n",
       " ('u57035589', 'g71655058'),\n",
       " ('u02211193', 'g61264440'),\n",
       " ('u02211193', 'g03200916'),\n",
       " ('u85269168', 'g18111735'),\n",
       " ('u85269168', 'g32904933'),\n",
       " ('u76404562', 'g81858269'),\n",
       " ('u76404562', 'g68460338'),\n",
       " ('u70389962', 'g76120150'),\n",
       " ('u70389962', 'g76099968'),\n",
       " ('u57877998', 'g71192260'),\n",
       " ('u57877998', 'g93882251'),\n",
       " ('u36098188', 'g17807252'),\n",
       " ('u36098188', 'g56626398'),\n",
       " ('u37555146', 'g26192910'),\n",
       " ('u37555146', 'g60509936'),\n",
       " ('u77668657', 'g84720805'),\n",
       " ('u77668657', 'g24194110'),\n",
       " ('u76625956', 'g65905047'),\n",
       " ('u76625956', 'g30761832'),\n",
       " ('u80541052', 'g51560220'),\n",
       " ('u80541052', 'g51573323'),\n",
       " ('u81055090', 'g89809648'),\n",
       " ('u81055090', 'g36958384'),\n",
       " ('u93072491', 'g74942396'),\n",
       " ('u93072491', 'g39394945'),\n",
       " ('u06746174', 'g26710111'),\n",
       " ('u06746174', 'g82616432'),\n",
       " ('u87165125', 'g27491100'),\n",
       " ('u87165125', 'g60267962'),\n",
       " ('u22286703', 'g79604681'),\n",
       " ('u22286703', 'g89194710'),\n",
       " ('u50086910', 'g63870321'),\n",
       " ('u50086910', 'g77718054'),\n",
       " ('u24610597', 'g35296108'),\n",
       " ('u24610597', 'g58646191'),\n",
       " ('u79381473', 'g79499736'),\n",
       " ('u79381473', 'g41577774'),\n",
       " ('u82943357', 'g24550764'),\n",
       " ('u82943357', 'g99551859'),\n",
       " ('u66004745', 'g68959096'),\n",
       " ('u66004745', 'g51450658'),\n",
       " ('u30207117', 'g62850718'),\n",
       " ('u30207117', 'g93027730'),\n",
       " ('u50584769', 'g09203234'),\n",
       " ('u50584769', 'g33823931'),\n",
       " ('u12502495', 'g66296428'),\n",
       " ('u12502495', 'g25844736'),\n",
       " ('u46075637', 'g48556857'),\n",
       " ('u46075637', 'g55070029'),\n",
       " ('u74896566', 'g17807252'),\n",
       " ('u74896566', 'g98908589'),\n",
       " ('u65883423', 'g78811835'),\n",
       " ('u65883423', 'g90836168'),\n",
       " ('u77351829', 'g43689131'),\n",
       " ('u77351829', 'g22948369'),\n",
       " ('u17884154', 'g60053245'),\n",
       " ('u17884154', 'g30982262'),\n",
       " ('u19075325', 'g23106667'),\n",
       " ('u19075325', 'g79181962'),\n",
       " ('u73735056', 'g83737994'),\n",
       " ('u73735056', 'g87757756'),\n",
       " ('u45635865', 'g79819183'),\n",
       " ('u45635865', 'g68576640'),\n",
       " ('u91862741', 'g78355069'),\n",
       " ('u91862741', 'g47729030'),\n",
       " ('u47732229', 'g92237283'),\n",
       " ('u47732229', 'g50937985'),\n",
       " ('u35871341', 'g47669117'),\n",
       " ('u35871341', 'g27466421'),\n",
       " ('u61482504', 'g67366154'),\n",
       " ('u61482504', 'g03280058'),\n",
       " ('u36368445', 'g53927674'),\n",
       " ('u36368445', 'g63309816'),\n",
       " ('u55093422', 'g10259243'),\n",
       " ('u55093422', 'g82275622'),\n",
       " ('u63686582', 'g88847485'),\n",
       " ('u63686582', 'g42403034'),\n",
       " ('u63229947', 'g15536656'),\n",
       " ('u63229947', 'g14368997'),\n",
       " ('u67143531', 'g48657523'),\n",
       " ('u67143531', 'g00187437'),\n",
       " ('u00791934', 'g51283481'),\n",
       " ('u00791934', 'g75450500'),\n",
       " ('u73273194', 'g17612425'),\n",
       " ('u73273194', 'g78566721'),\n",
       " ('u51062008', 'g79333823'),\n",
       " ('u51062008', 'g00131880'),\n",
       " ('u20566880', 'g17604638'),\n",
       " ('u20566880', 'g17981761'),\n",
       " ('u24139989', 'g60355697'),\n",
       " ('u24139989', 'g48662041'),\n",
       " ('u99615598', 'g97650885'),\n",
       " ('u99615598', 'g85318104'),\n",
       " ('u51872234', 'g91177309'),\n",
       " ('u51872234', 'g99613754'),\n",
       " ('u04695380', 'g85442587'),\n",
       " ('u04695380', 'g53302271'),\n",
       " ('u50987399', 'g05572229'),\n",
       " ('u50987399', 'g89194710'),\n",
       " ('u69158652', 'g38436583'),\n",
       " ('u69158652', 'g24805497'),\n",
       " ('u90087391', 'g60355697'),\n",
       " ('u90087391', 'g90301006'),\n",
       " ('u26800137', 'g97784906'),\n",
       " ('u26800137', 'g22832209'),\n",
       " ('u54511224', 'g24550764'),\n",
       " ('u54511224', 'g96830890'),\n",
       " ('u10803498', 'g30891154'),\n",
       " ('u10803498', 'g26654626'),\n",
       " ('u83845547', 'g15371817'),\n",
       " ('u83845547', 'g21883457'),\n",
       " ('u26810316', 'g29592534'),\n",
       " ('u26810316', 'g17712832'),\n",
       " ('u57508791', 'g41348232'),\n",
       " ('u57508791', 'g81860955'),\n",
       " ('u67932163', 'g17361564'),\n",
       " ('u67932163', 'g49167205'),\n",
       " ('u46440137', 'g99526401'),\n",
       " ('u46440137', 'g83843155'),\n",
       " ('u46422197', 'g08561999'),\n",
       " ('u46422197', 'g00102027'),\n",
       " ('u50754309', 'g67429053'),\n",
       " ('u50754309', 'g85484727'),\n",
       " ('u79289617', 'g62805978'),\n",
       " ('u79289617', 'g28940912'),\n",
       " ('u81058112', 'g96083451'),\n",
       " ('u81058112', 'g89055558'),\n",
       " ('u26246234', 'g70906782'),\n",
       " ('u26246234', 'g68420270'),\n",
       " ('u45713244', 'g82220080'),\n",
       " ('u45713244', 'g40725802'),\n",
       " ('u03606376', 'g75307137'),\n",
       " ('u03606376', 'g80035010'),\n",
       " ('u31712305', 'g81608348'),\n",
       " ('u31712305', 'g90379019'),\n",
       " ('u29861752', 'g70011042'),\n",
       " ('u29861752', 'g11874212'),\n",
       " ('u72478974', 'g22284227'),\n",
       " ('u72478974', 'g71312034'),\n",
       " ('u85801219', 'g19179172'),\n",
       " ('u85801219', 'g97038061'),\n",
       " ('u96733126', 'g16246767'),\n",
       " ('u96733126', 'g06686363'),\n",
       " ('u80880312', 'g60949442'),\n",
       " ('u80880312', 'g89997386'),\n",
       " ('u78875726', 'g58479043'),\n",
       " ('u78875726', 'g52740278'),\n",
       " ('u45884104', 'g67038900'),\n",
       " ('u45884104', 'g23762514'),\n",
       " ('u55525583', 'g82317851'),\n",
       " ('u55525583', 'g71838956'),\n",
       " ('u17061439', 'g95765068'),\n",
       " ('u17061439', 'g06819571'),\n",
       " ('u79483995', 'g66296428'),\n",
       " ('u79483995', 'g50937985'),\n",
       " ('u26022408', 'g15714062'),\n",
       " ('u26022408', 'g05863698'),\n",
       " ('u54274967', 'g75447996'),\n",
       " ('u54274967', 'g21285355'),\n",
       " ('u11977499', 'g39366834'),\n",
       " ('u11977499', 'g24809309'),\n",
       " ('u79621856', 'g77859458'),\n",
       " ('u79621856', 'g38024861'),\n",
       " ('u05925641', 'g82874353'),\n",
       " ('u05925641', 'g75248233'),\n",
       " ('u32383577', 'g58649509'),\n",
       " ('u32383577', 'g90538928'),\n",
       " ('u90013638', 'g70709452'),\n",
       " ('u90013638', 'g12460730'),\n",
       " ('u35345442', 'g57843499'),\n",
       " ('u35345442', 'g13018310'),\n",
       " ('u01768310', 'g05463839'),\n",
       " ('u01768310', 'g88847485'),\n",
       " ('u03233576', 'g25612121'),\n",
       " ('u03233576', 'g90301006'),\n",
       " ('u58504548', 'g37782017'),\n",
       " ('u58504548', 'g84269623'),\n",
       " ('u63606659', 'g37899539'),\n",
       " ('u63606659', 'g79686556'),\n",
       " ('u61237602', 'g79499736'),\n",
       " ('u61237602', 'g33108407'),\n",
       " ('u52662370', 'g54634449'),\n",
       " ('u52662370', 'g51491804'),\n",
       " ('u18556594', 'g90898833'),\n",
       " ('u18556594', 'g30838069'),\n",
       " ('u62365327', 'g36958384'),\n",
       " ('u62365327', 'g45124396'),\n",
       " ('u50936984', 'g00657913'),\n",
       " ('u50936984', 'g92782480'),\n",
       " ('u74137551', 'g24705380'),\n",
       " ('u74137551', 'g32641915'),\n",
       " ('u55294871', 'g19831304'),\n",
       " ('u55294871', 'g26730287'),\n",
       " ('u09660080', 'g93969631'),\n",
       " ('u09660080', 'g02337832'),\n",
       " ('u79355096', 'g01086845'),\n",
       " ('u79355096', 'g05572229'),\n",
       " ('u02129594', 'g35155801'),\n",
       " ('u02129594', 'g03916495'),\n",
       " ('u80064824', 'g92468669'),\n",
       " ('u80064824', 'g36841558'),\n",
       " ('u03749090', 'g00083675'),\n",
       " ('u03749090', 'g76099968'),\n",
       " ('u51581636', 'g12946119'),\n",
       " ('u51581636', 'g91035389'),\n",
       " ('u26345786', 'g88216046'),\n",
       " ('u26345786', 'g51322209'),\n",
       " ('u27754940', 'g89627546'),\n",
       " ('u27754940', 'g36060349'),\n",
       " ('u82727393', 'g92522857'),\n",
       " ('u82727393', 'g64432243'),\n",
       " ('u82999931', 'g32989876'),\n",
       " ('u82999931', 'g10701910'),\n",
       " ('u47661995', 'g71822589'),\n",
       " ('u47661995', 'g84150274'),\n",
       " ('u66568694', 'g15035813'),\n",
       " ('u66568694', 'g15050817'),\n",
       " ('u84107660', 'g65055990'),\n",
       " ('u84107660', 'g38440799'),\n",
       " ('u47129563', 'g38914136'),\n",
       " ('u47129563', 'g06235078'),\n",
       " ('u38871576', 'g28788625'),\n",
       " ('u38871576', 'g48753510'),\n",
       " ('u93683322', 'g18223055'),\n",
       " ('u93683322', 'g19856409'),\n",
       " ('u73487372', 'g41331921'),\n",
       " ('u73487372', 'g92569712'),\n",
       " ('u83427263', 'g75513928'),\n",
       " ('u83427263', 'g46425800'),\n",
       " ('u26933725', 'g69693461'),\n",
       " ('u26933725', 'g65041719'),\n",
       " ('u09703080', 'g08020339'),\n",
       " ('u09703080', 'g48452334'),\n",
       " ('u03431726', 'g90301006'),\n",
       " ('u03431726', 'g31948580'),\n",
       " ('u31245260', 'g35073657'),\n",
       " ('u31245260', 'g17804798'),\n",
       " ('u30730456', 'g95842407'),\n",
       " ('u30730456', 'g95421853'),\n",
       " ('u17280357', 'g07047848'),\n",
       " ('u17280357', 'g14523030'),\n",
       " ('u92381286', 'g03280058'),\n",
       " ('u92381286', 'g42142445'),\n",
       " ('u66376398', 'g64568215'),\n",
       " ('u66376398', 'g00290491'),\n",
       " ('u53134535', 'g35549655'),\n",
       " ('u53134535', 'g84254492'),\n",
       " ('u56788026', 'g83425645'),\n",
       " ('u56788026', 'g59848866'),\n",
       " ('u18506333', 'g80979867'),\n",
       " ('u18506333', 'g88216046'),\n",
       " ('u42100273', 'g40485117'),\n",
       " ('u42100273', 'g27370047'),\n",
       " ('u22346040', 'g00602342'),\n",
       " ('u22346040', 'g13343607'),\n",
       " ('u17484711', 'g85642901'),\n",
       " ('u17484711', 'g49208981'),\n",
       " ('u61683676', 'g39930645'),\n",
       " ('u61683676', 'g84951444'),\n",
       " ('u27205339', 'g98118815'),\n",
       " ('u27205339', 'g09383924'),\n",
       " ('u11960893', 'g02428379'),\n",
       " ('u11960893', 'g56500643'),\n",
       " ('u54654852', 'g89492775'),\n",
       " ('u54654852', 'g62370166'),\n",
       " ('u66968893', 'g53223749'),\n",
       " ('u66968893', 'g85692281'),\n",
       " ('u03085578', 'g96831210'),\n",
       " ('u03085578', 'g13451594'),\n",
       " ('u32706320', 'g59317800'),\n",
       " ('u32706320', 'g88607112'),\n",
       " ('u23484702', 'g75228197'),\n",
       " ('u23484702', 'g05914065'),\n",
       " ('u95036303', 'g75498630'),\n",
       " ('u95036303', 'g72683624'),\n",
       " ('u51545733', 'g03420056'),\n",
       " ('u51545733', 'g45963204'),\n",
       " ('u04759919', 'g35984506'),\n",
       " ('u04759919', 'g89627546'),\n",
       " ('u70160774', 'g45468788'),\n",
       " ('u70160774', 'g57187406'),\n",
       " ('u81349942', 'g84476497'),\n",
       " ('u81349942', 'g13449021'),\n",
       " ('u14125630', 'g65055990'),\n",
       " ('u14125630', 'g46101992'),\n",
       " ('u88835772', 'g90427081'),\n",
       " ('u88835772', 'g33108407'),\n",
       " ('u39532419', 'g42142445'),\n",
       " ('u39532419', 'g44908961'),\n",
       " ('u62203024', 'g74412462'),\n",
       " ('u62203024', 'g13226978'),\n",
       " ('u36137576', 'g35331676'),\n",
       " ('u36137576', 'g16369263'),\n",
       " ('u81914083', 'g02637258'),\n",
       " ('u81914083', 'g98217589'),\n",
       " ('u04430455', 'g38024861'),\n",
       " ('u04430455', 'g03136155'),\n",
       " ('u17513326', 'g82616432'),\n",
       " ('u17513326', 'g06322315'),\n",
       " ('u33316686', 'g17807252'),\n",
       " ('u33316686', 'g67429053'),\n",
       " ('u90733484', 'g91879395'),\n",
       " ('u90733484', 'g92694999'),\n",
       " ('u78081211', 'g16586637'),\n",
       " ('u78081211', 'g09409361'),\n",
       " ('u79131192', 'g46602286'),\n",
       " ('u79131192', 'g25051247'),\n",
       " ('u31201511', 'g38724809'),\n",
       " ('u31201511', 'g79861485'),\n",
       " ('u01822396', 'g34397868'),\n",
       " ('u01822396', 'g89998902'),\n",
       " ('u45818240', 'g90581288'),\n",
       " ('u45818240', 'g75142381'),\n",
       " ('u26102740', 'g71795407'),\n",
       " ('u26102740', 'g47344447'),\n",
       " ('u75222770', 'g99830215'),\n",
       " ('u75222770', 'g65041719'),\n",
       " ('u55344934', 'g44130855'),\n",
       " ('u55344934', 'g53041186'),\n",
       " ('u83206928', 'g35984506'),\n",
       " ('u83206928', 'g70822619'),\n",
       " ('u37501198', 'g83221585'),\n",
       " ('u37501198', 'g44078228'),\n",
       " ('u63952530', 'g73594265'),\n",
       " ('u63952530', 'g00288786'),\n",
       " ('u09796001', 'g08143802'),\n",
       " ('u09796001', 'g81860955'),\n",
       " ('u38860000', 'g01660600'),\n",
       " ('u38860000', 'g26937993'),\n",
       " ('u25087987', 'g86213381'),\n",
       " ('u25087987', 'g27495791'),\n",
       " ('u70734575', 'g54634449'),\n",
       " ('u70734575', 'g76822730'),\n",
       " ('u03124291', 'g75160975'),\n",
       " ('u03124291', 'g35449408'),\n",
       " ('u89482132', 'g53996086'),\n",
       " ('u89482132', 'g99317241'),\n",
       " ('u30530866', 'g63870321'),\n",
       " ('u30530866', 'g15090160'),\n",
       " ('u89855328', 'g69433247'),\n",
       " ('u89855328', 'g29695966'),\n",
       " ('u75148448', 'g30541991'),\n",
       " ('u75148448', 'g84720805'),\n",
       " ('u82868412', 'g48753510'),\n",
       " ('u82868412', 'g51753734'),\n",
       " ('u09621573', 'g35449408'),\n",
       " ('u09621573', 'g12104166'),\n",
       " ('u64934997', 'g83517324'),\n",
       " ('u64934997', 'g26111343'),\n",
       " ('u44065947', 'g39055346'),\n",
       " ('u44065947', 'g22948369'),\n",
       " ('u57508791', 'g86195002'),\n",
       " ('u57508791', 'g48512616'),\n",
       " ('u02211193', 'g74469956'),\n",
       " ('u02211193', 'g62348708'),\n",
       " ('u29615491', 'g47205010'),\n",
       " ('u29615491', 'g72028177'),\n",
       " ('u21483313', 'g35549655'),\n",
       " ('u21483313', 'g45508008'),\n",
       " ('u15930213', 'g99526401'),\n",
       " ('u15930213', 'g33702735'),\n",
       " ('u07426800', 'g16073667'),\n",
       " ('u07426800', 'g71655058'),\n",
       " ('u21451966', 'g89492775'),\n",
       " ('u21451966', 'g29741733'),\n",
       " ('u90742229', 'g78239118'),\n",
       " ('u90742229', 'g58521117'),\n",
       " ('u15010009', 'g14867677'),\n",
       " ('u15010009', 'g24274307'),\n",
       " ('u85865115', 'g10126403'),\n",
       " ('u85865115', 'g97435957'),\n",
       " ('u29006683', 'g26556480'),\n",
       " ('u29006683', 'g04443491'),\n",
       " ('u20194412', 'g11862712'),\n",
       " ('u20194412', 'g34026161'),\n",
       " ('u70628300', 'g93941352'),\n",
       " ('u70628300', 'g01959969'),\n",
       " ('u92437348', 'g35331676'),\n",
       " ('u92437348', 'g01086845'),\n",
       " ('u64463975', 'g97014047'),\n",
       " ('u64463975', 'g33823931'),\n",
       " ('u05548301', 'g91879395'),\n",
       " ('u05548301', 'g05982385'),\n",
       " ('u00584629', 'g97528772'),\n",
       " ('u00584629', 'g58314864'),\n",
       " ('u49961321', 'g68135850'),\n",
       " ('u49961321', 'g48341630'),\n",
       " ('u09842258', 'g16320816'),\n",
       " ('u09842258', 'g08347342'),\n",
       " ('u70694256', 'g70232376'),\n",
       " ('u70694256', 'g92794719'),\n",
       " ('u17081932', 'g65794132'),\n",
       " ('u17081932', 'g97931337'),\n",
       " ('u62201021', 'g45328688'),\n",
       " ('u62201021', 'g51812669'),\n",
       " ('u11760972', 'g85021436'),\n",
       " ('u11760972', 'g78537882'),\n",
       " ('u22185524', 'g69577828'),\n",
       " ('u22185524', 'g31800990'),\n",
       " ('u13941749', 'g43636148'),\n",
       " ('u13941749', 'g47086763'),\n",
       " ('u27257486', 'g05463839'),\n",
       " ('u27257486', 'g09383924'),\n",
       " ('u88078966', 'g17807252'),\n",
       " ('u88078966', 'g54506405'),\n",
       " ('u98268996', 'g98707416'),\n",
       " ('u98268996', 'g32095755'),\n",
       " ('u40780359', 'g52540758'),\n",
       " ('u40780359', 'g41292835'),\n",
       " ('u22509254', 'g99830215'),\n",
       " ('u22509254', 'g82441703'),\n",
       " ('u26046733', 'g66086214'),\n",
       " ('u26046733', 'g95658921'),\n",
       " ('u65501677', 'g83607000'),\n",
       " ('u65501677', 'g44038209'),\n",
       " ('u17313321', 'g61264440'),\n",
       " ('u17313321', 'g64167567'),\n",
       " ('u77305148', 'g29494592'),\n",
       " ('u77305148', 'g78034556'),\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'g61913894' in gamesPerUser['u00914251']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_q1 = []\n",
    "pred_q1 = []\n",
    "\n",
    "for pair in newValid:\n",
    "    uid = pair[0]\n",
    "    gid = pair[1]\n",
    "\n",
    "    if gid in gamesPerUser[uid]:\n",
    "        actual_q1.append(1)\n",
    "    else:\n",
    "        actual_q1.append(0)\n",
    "\n",
    "    if gid in return1:\n",
    "        pred_q1.append(1)\n",
    "    else:\n",
    "        pred_q1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = 0\n",
    "\n",
    "for i in range(len(actual_q1)):\n",
    "    if actual_q1[i] == pred_q1[i]:\n",
    "        matches += 1\n",
    "\n",
    "acc_q1 = matches / len(actual_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17cb78c4-5841-46a9-af75-cc347d4f39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = acc_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "158deaa6-d294-4873-b10f-85f883d833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f843a2a7-57e5-4947-a513-ba8fa35f8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47c2b474-700f-4d37-be1b-3a704ad2968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor: 0.35, acc: 0.6454145414541455\n",
      "factor: 0.4, acc: 0.6603160316031603\n",
      "factor: 0.45, acc: 0.6745674567456745\n",
      "factor: 0.5, acc: 0.683968396839684\n",
      "factor: 0.55, acc: 0.6954195419541954\n"
     ]
    }
   ],
   "source": [
    "# Improved strategy\n",
    "\n",
    "def filterMostPopular(factor=1/2):\n",
    "    output = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        output.add(i)\n",
    "        if count > totalPlayed*factor: \n",
    "            return output\n",
    "\n",
    "factors = [0.35, 0.40, 0.45, 0.5, 0.55]\n",
    "for f in factors:\n",
    "    return1 = filterMostPopular(f)\n",
    "\n",
    "    pred = []\n",
    "    for pair in newValid:\n",
    "        uid = pair[0]\n",
    "        gid = pair[1]\n",
    "\n",
    "        if gid in return1:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "\n",
    "    matches = 0\n",
    "    for i in range(len(actual_q1)):\n",
    "        if actual_q1[i] == pred[i]:\n",
    "            matches += 1\n",
    "\n",
    "    acc = matches / len(actual_q1)\n",
    "    print(f\"factor: {f}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6474d5ac-7dd0-4d62-b938-ec025bef55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline strategy\n",
    "\n",
    "return1 = filterMostPopular(0.55)\n",
    "\n",
    "pred_q2 = []\n",
    "for pair in newValid:\n",
    "    uid = pair[0]\n",
    "    gid = pair[1]\n",
    "\n",
    "    if gid in return1:\n",
    "        pred_q2.append(1)\n",
    "    else:\n",
    "        pred_q2.append(0)\n",
    "\n",
    "matches = 0\n",
    "for i in range(len(actual_q1)):\n",
    "    if actual_q1[i] == pred_q2[i]:\n",
    "        matches += 1\n",
    "\n",
    "acc_q2 = matches / len(actual_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06a69839-1423-4160-80dc-451eccf6b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [totalPlayed*0.55, acc_q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684, 'Q2': [96249.45000000001, 0.6954195419541954]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44ddabf1-bf18-428d-91b2-82702133cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c5c5e95-1c35-4f00-9fac-5a1d3bec338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e7cca75-8730-459c-ad27-d827d65856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful data structures\n",
    "\n",
    "playersPerGame = defaultdict(set)\n",
    "gamesPerPlayer = defaultdict(set)\n",
    "\n",
    "for h in hoursTrain:\n",
    "    uid = h[0]\n",
    "    gid = h[1]\n",
    "    playersPerGame[gid].add(uid)\n",
    "    gamesPerPlayer[uid].add(gid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q3 = []\n",
    "all_sim = []\n",
    "\n",
    "for h in newValid:\n",
    "    uid = h[0]\n",
    "    gid = h[1]\n",
    "    played = gamesPerPlayer[uid]\n",
    "    similarities = []\n",
    "    for gid2 in played:\n",
    "        sim = Jaccard(playersPerGame[gid], playersPerGame[gid2])\n",
    "        similarities.append(sim)\n",
    "        all_sim.append(sim)\n",
    "    if max(similarities) > 0.030:\n",
    "        pred_q3.append(1)\n",
    "    else: \n",
    "        pred_q3.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(all_sim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>736466.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.015909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.148936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarities\n",
       "count  736466.000000\n",
       "mean        0.010517\n",
       "std         0.011891\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.007407\n",
       "75%         0.015909\n",
       "max         0.148936"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = pd.DataFrame(data={'similarities': all_sim})\n",
    "dummy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 accuracy\n",
    "\n",
    "matches = 0\n",
    "for i in range(len(actual_q1)):\n",
    "    if actual_q1[i] == pred_q3[i]:\n",
    "        matches += 1\n",
    "\n",
    "acc_q3 = matches / len(actual_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673067306730673"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q4 = []\n",
    "\n",
    "return1 = filterMostPopular(0.55)\n",
    "\n",
    "for h in newValid:\n",
    "    uid = h[0]\n",
    "    gid = h[1]\n",
    "    played = gamesPerPlayer[uid]\n",
    "    similarities = []\n",
    "    for gid2 in played:\n",
    "        sim = Jaccard(playersPerGame[gid], playersPerGame[gid2])\n",
    "        similarities.append(sim)\n",
    "    if max(similarities) > 0.03 and gid in return1:\n",
    "        pred_q4.append(1)\n",
    "    else: \n",
    "        pred_q4.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 accuracy\n",
    "\n",
    "matches = 0\n",
    "for i in range(len(actual_q1)):\n",
    "    if actual_q1[i] == pred_q4[i]:\n",
    "        matches += 1\n",
    "\n",
    "acc_q4 = matches / len(actual_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6180d5a7-fcaa-4208-9e2e-0babf0ab854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = acc_q3\n",
    "answers['Q4'] = acc_q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ac29b20-93d8-467e-9343-7363ae7c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q3'])\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684,\n",
       " 'Q2': [96249.45000000001, 0.6954195419541954],\n",
       " 'Q3': 0.673067306730673,\n",
       " 'Q4': 0.6867186718671867}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75f81286-487d-494a-8ee8-a42c1aca6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = filterMostPopular(0.55)\n",
    "predictions = open(\"HWpredictions_Played.csv\", 'w')\n",
    "for l in open(\"data/pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    # Logic...\n",
    "    played = gamesPerPlayer[u]\n",
    "    similarities = [0]\n",
    "    for g2 in played:\n",
    "        sim = Jaccard(playersPerGame[g], playersPerGame[g2])\n",
    "        similarities.append(sim)\n",
    "    if max(similarities) > 0.03 and g in return1:\n",
    "        pred = 1\n",
    "    else: \n",
    "        pred = 0\n",
    "    \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbfd2cbf-b515-4f70-b613-e1248c5d6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684,\n",
       " 'Q2': [96249.45000000001, 0.6954195419541954],\n",
       " 'Q3': 0.673067306730673,\n",
       " 'Q4': 0.6867186718671867,\n",
       " 'Q5': 'I confirm that I have uploaded an assignment submission to gradescope'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c82a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hours played prediction                        #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "772dd561-ceae-4c2e-9347-7ba3eb2dd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHours = [r[2]['hours_transformed'] for r in hoursTrain]\n",
    "globalAverage = sum(trainHours) * 1.0 / len(trainHours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b95a9e5-b36f-4883-befb-6dedfd833dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursPerUser = defaultdict(list)\n",
    "hoursPerItem = defaultdict(list)\n",
    "\n",
    "for h in hoursTrain:\n",
    "    uid = h[0]\n",
    "    gid = h[1]\n",
    "    hoursTransformed = h[2]['hours_transformed']\n",
    "    hoursPerUser[uid].append(hoursTransformed)\n",
    "    hoursPerItem[gid].append(hoursTransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.356714460537184"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaU['u91746794']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae174441-3c7e-4b41-8869-7a67b6c61607",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = globalAverage # Could initialize anywhere, this is a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2893185278431027"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(betaU.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(actual, pred):\n",
    "    return np.sum((actual-pred)**2) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = globalAverage # Could initialize anywhere, this is a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in hoursPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for g in hoursPerItem:\n",
    "    betaI[g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHoursByPair = defaultdict(float)\n",
    "itemsPerUser = defaultdict(set)\n",
    "usersPerItem = defaultdict(set)\n",
    "\n",
    "for h in hoursTrain:\n",
    "    uid, gid, hours = h[0], h[1], h[2]['hours_transformed']\n",
    "    trainHoursByPair[(uid, gid)] = hours\n",
    "    itemsPerUser[uid].add(gid)\n",
    "    usersPerItem[gid].add(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha function\n",
    "def calculate_alpha():\n",
    "    numer = 0\n",
    "    for pair in trainHoursByPair:\n",
    "        u, g = pair[0], pair[1]\n",
    "        numer += (trainHoursByPair[pair]-(betaU[u]+betaI[g]))\n",
    "    denom = len(trainHours)\n",
    "    return numer / denom\n",
    "\n",
    "# BetaU function\n",
    "def calculate_betaU(u, alpha, lamb):\n",
    "    numer = 0\n",
    "    for i in itemsPerUser[u]:\n",
    "        numer += (trainHoursByPair[(u, i)]-(alpha+betaI[i]))\n",
    "    denom = lamb + len(itemsPerUser[u])\n",
    "    return numer / denom\n",
    "\n",
    "# BetaI function\n",
    "def calculate_betaI(i, alpha, lamb):\n",
    "    numer = 0\n",
    "    for u in usersPerItem[i]:\n",
    "        numer += (trainHoursByPair[(u, i)]-(alpha+betaU[u]))\n",
    "    denom = lamb + len(usersPerItem[i])\n",
    "    return numer / denom\n",
    "\n",
    "# Objective function\n",
    "def calculate_objective(alpha, lamb):\n",
    "    totalError = 0\n",
    "\n",
    "    for pair in trainHoursByPair:\n",
    "        pred = alpha + betaU[pair[0]] + betaI[pair[1]]\n",
    "        actual = trainHoursByPair[pair]\n",
    "        totalError += ((pred-actual)**2)\n",
    "\n",
    "    regularizer = lamb*\\\n",
    "        (np.sum(np.array(list(betaU.values()))**2) + \\\n",
    "        np.sum(np.array(list(betaI.values()))**2))\n",
    "        \n",
    "    return totalError + regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(lamb):\n",
    "\n",
    "    # calculate alpha\n",
    "    alpha_cd = calculate_alpha()\n",
    "\n",
    "    # loop over users, calculate betaU, store in dictionary\n",
    "    for p in hoursPerUser:\n",
    "        bU_cd = calculate_betaU(p, alpha_cd, lamb)\n",
    "        betaU[p] = bU_cd\n",
    "\n",
    "    # loop over items, calculate betaI, store in dictionary\n",
    "    for g in hoursPerItem:\n",
    "        bI_cd = calculate_betaI(g, alpha_cd, lamb)\n",
    "        betaI[g] = bI_cd\n",
    "\n",
    "    return alpha_cd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 494437.2516434087\n",
      "Iteration 2: Loss = 463123.9397682811\n",
      "Iteration 3: Loss = 461721.85359065386\n",
      "Iteration 4: Loss = 461607.19664067717\n",
      "Iteration 5: Loss = 461577.1498817906\n",
      "Iteration 6: Loss = 461553.6017012964\n",
      "Iteration 7: Loss = 461531.0891837214\n",
      "Iteration 8: Loss = 461509.2097913683\n",
      "Iteration 9: Loss = 461487.9204430444\n",
      "Iteration 10: Loss = 461467.20410987776\n",
      "Iteration 11: Loss = 461447.04596912133\n",
      "Iteration 12: Loss = 461427.43167515966\n",
      "Iteration 13: Loss = 461408.34722511936\n",
      "Iteration 14: Loss = 461389.77894268726\n",
      "Iteration 15: Loss = 461371.7134712492\n",
      "Iteration 16: Loss = 461354.1377680529\n",
      "Iteration 17: Loss = 461337.0390985415\n",
      "Iteration 18: Loss = 461320.40503067663\n",
      "Iteration 19: Loss = 461304.22342929954\n",
      "Iteration 20: Loss = 461288.4824504819\n",
      "Iteration 21: Loss = 461273.1705359054\n",
      "Iteration 22: Loss = 461258.2764073616\n",
      "Iteration 23: Loss = 461243.78906110453\n",
      "Iteration 24: Loss = 461229.69776245684\n",
      "Iteration 25: Loss = 461215.99204024643\n",
      "Iteration 26: Loss = 461202.66168144066\n",
      "Iteration 27: Loss = 461189.6967258225\n",
      "Iteration 28: Loss = 461177.0874606179\n",
      "Iteration 29: Loss = 461164.8244152757\n",
      "Iteration 30: Loss = 461152.8983563221\n",
      "Iteration 31: Loss = 461141.3002821104\n",
      "Iteration 32: Loss = 461130.02141793416\n",
      "Iteration 33: Loss = 461119.0532109089\n",
      "Iteration 34: Loss = 461108.3873250996\n",
      "Iteration 35: Loss = 461098.01563663327\n",
      "Iteration 36: Loss = 461087.9302290129\n",
      "Iteration 37: Loss = 461078.12338828744\n",
      "Iteration 38: Loss = 461068.58759851306\n",
      "Iteration 39: Loss = 461059.31553716026\n",
      "Iteration 40: Loss = 461050.30007063254\n",
      "Iteration 41: Loss = 461041.5342498998\n",
      "Iteration 42: Loss = 461033.01130611013\n",
      "Iteration 43: Loss = 461024.7246464253\n",
      "Iteration 44: Loss = 461016.66784973507\n",
      "Iteration 45: Loss = 461008.8346626618\n",
      "Iteration 46: Loss = 461001.2189955302\n",
      "Iteration 47: Loss = 460993.81491832185\n",
      "Iteration 48: Loss = 460986.61665697757\n",
      "Iteration 49: Loss = 460979.61858945887\n",
      "Iteration 50: Loss = 460972.81524211616\n",
      "Iteration 51: Loss = 460966.2012860058\n",
      "Iteration 52: Loss = 460959.77153336373\n",
      "Iteration 53: Loss = 460953.52093409235\n",
      "Iteration 54: Loss = 460947.4445723503\n",
      "Iteration 55: Loss = 460941.5376631661\n",
      "Iteration 56: Loss = 460935.7955492336\n",
      "Iteration 57: Loss = 460930.21369765693\n",
      "Iteration 58: Loss = 460924.7876968366\n",
      "Iteration 59: Loss = 460919.51325339836\n",
      "Iteration 60: Loss = 460914.3861891538\n",
      "Iteration 61: Loss = 460909.4024382721\n",
      "Iteration 62: Loss = 460904.5580443088\n",
      "Iteration 63: Loss = 460899.849157467\n",
      "Iteration 64: Loss = 460895.2720318248\n",
      "Iteration 65: Loss = 460890.8230226908\n",
      "Iteration 66: Loss = 460886.4985839671\n",
      "Iteration 67: Loss = 460882.2952656482\n",
      "Iteration 68: Loss = 460878.20971122576\n",
      "Iteration 69: Loss = 460874.2386553852\n",
      "Iteration 70: Loss = 460870.37892153155\n",
      "Iteration 71: Loss = 460866.627419534\n",
      "Iteration 72: Loss = 460862.9811434038\n",
      "Iteration 73: Loss = 460859.43716915115\n",
      "Iteration 74: Loss = 460855.9926525827\n",
      "Iteration 75: Loss = 460852.6448272152\n",
      "Iteration 76: Loss = 460849.39100219175\n",
      "Iteration 77: Loss = 460846.2285603904\n",
      "Iteration 78: Loss = 460843.15495633404\n",
      "Iteration 79: Loss = 460840.167714342\n",
      "Iteration 80: Loss = 460837.264426707\n",
      "Iteration 81: Loss = 460834.44275185466\n",
      "Iteration 82: Loss = 460831.70041256264\n",
      "Iteration 83: Loss = 460829.035194299\n",
      "Iteration 84: Loss = 460826.44494343764\n",
      "Iteration 85: Loss = 460823.927565774\n",
      "Iteration 86: Loss = 460821.48102479323\n",
      "Iteration 87: Loss = 460819.1033401833\n",
      "Iteration 88: Loss = 460816.79258635367\n",
      "Iteration 89: Loss = 460814.54689084523\n",
      "Iteration 90: Loss = 460812.3644330419\n",
      "Iteration 91: Loss = 460810.24344267196\n",
      "Iteration 92: Loss = 460808.1821984452\n",
      "Iteration 93: Loss = 460806.1790268074\n",
      "Iteration 94: Loss = 460804.23230054806\n",
      "Iteration 95: Loss = 460802.3404375971\n",
      "Iteration 96: Loss = 460800.50189979444\n",
      "Iteration 97: Loss = 460798.7151916594\n",
      "Iteration 98: Loss = 460796.9788592767\n",
      "Iteration 99: Loss = 460795.2914891535\n",
      "Iteration 100: Loss = 460793.65170704474\n",
      "Iteration 101: Loss = 460792.0581769799\n",
      "Iteration 102: Loss = 460790.50960017496\n",
      "Iteration 103: Loss = 460789.0047140119\n",
      "Iteration 104: Loss = 460787.54229104816\n",
      "Iteration 105: Loss = 460786.1211380176\n",
      "Iteration 106: Loss = 460784.74009496777\n",
      "Iteration 107: Loss = 460783.3980342826\n",
      "Iteration 108: Loss = 460782.09385983297\n",
      "Iteration 109: Loss = 460780.82650607836\n",
      "Iteration 110: Loss = 460779.5949372385\n",
      "Iteration 111: Loss = 460778.3981464623\n",
      "Iteration 112: Loss = 460777.23515506287\n",
      "Iteration 113: Loss = 460776.1050117088\n",
      "Iteration 114: Loss = 460775.00679165235\n",
      "Iteration 115: Loss = 460773.9395960465\n",
      "Iteration 116: Loss = 460772.9025511972\n",
      "Iteration 117: Loss = 460771.89480788575\n",
      "Iteration 118: Loss = 460770.91554064094\n",
      "Iteration 119: Loss = 460769.96394713473\n",
      "Iteration 120: Loss = 460769.0392475434\n",
      "Iteration 121: Loss = 460768.1406838757\n",
      "Iteration 122: Loss = 460767.2675193921\n",
      "Iteration 123: Loss = 460766.41903803713\n",
      "Iteration 124: Loss = 460765.5945438385\n",
      "Iteration 125: Loss = 460764.7933603343\n",
      "Iteration 126: Loss = 460764.0148301049\n",
      "Iteration 127: Loss = 460763.2583140856\n",
      "Iteration 128: Loss = 460762.52319128724\n",
      "Iteration 129: Loss = 460761.80885808135\n",
      "Iteration 130: Loss = 460761.11472780455\n",
      "Iteration 131: Loss = 460760.44023031305\n",
      "Iteration 132: Loss = 460759.7848114793\n",
      "Iteration 133: Loss = 460759.14793271717\n",
      "Iteration 134: Loss = 460758.5290706434\n",
      "Iteration 135: Loss = 460757.9277165555\n",
      "Iteration 136: Loss = 460757.34337610303\n",
      "Iteration 137: Loss = 460756.775568769\n",
      "Iteration 138: Loss = 460756.2238276607\n",
      "Iteration 139: Loss = 460755.68769895524\n",
      "Iteration 140: Loss = 460755.1667416293\n",
      "Iteration 141: Loss = 460754.6605270802\n",
      "Iteration 142: Loss = 460754.1686387595\n",
      "Iteration 143: Loss = 460753.6906718945\n",
      "Iteration 144: Loss = 460753.2262330624\n",
      "Iteration 145: Loss = 460752.7749399755\n",
      "Iteration 146: Loss = 460752.3364210621\n",
      "Iteration 147: Loss = 460751.9103152821\n",
      "Iteration 148: Loss = 460751.49627174664\n",
      "Iteration 149: Loss = 460751.09394945874\n",
      "Iteration 150: Loss = 460750.70301703014\n",
      "Iteration 151: Loss = 460750.3231524479\n",
      "Iteration 152: Loss = 460749.95404274383\n",
      "Iteration 153: Loss = 460749.5953838063\n",
      "Iteration 154: Loss = 460749.2468800949\n",
      "Iteration 155: Loss = 460748.9082443954\n",
      "Iteration 156: Loss = 460748.5791976267\n",
      "Iteration 157: Loss = 460748.259468548\n",
      "Iteration 158: Loss = 460747.94879361894\n",
      "Iteration 159: Loss = 460747.6469166883\n",
      "Iteration 160: Loss = 460747.35358887166\n",
      "Iteration 161: Loss = 460747.0685683176\n",
      "Iteration 162: Loss = 460746.7916199752\n",
      "Iteration 163: Loss = 460746.52251545375\n",
      "Iteration 164: Loss = 460746.2610327941\n",
      "Iteration 165: Loss = 460746.0069563302\n",
      "Iteration 166: Loss = 460745.7600764779\n",
      "Iteration 167: Loss = 460745.520189562\n",
      "Iteration 168: Loss = 460745.2870976817\n",
      "Iteration 169: Loss = 460745.0606085107\n",
      "Iteration 170: Loss = 460744.84053517616\n",
      "Iteration 171: Loss = 460744.6266960907\n",
      "Iteration 172: Loss = 460744.41891473206\n",
      "Iteration 173: Loss = 460744.2170197038\n",
      "Iteration 174: Loss = 460744.0208442884\n",
      "Iteration 175: Loss = 460743.83022662817\n",
      "Iteration 176: Loss = 460743.6450093454\n",
      "Iteration 177: Loss = 460743.46503954194\n",
      "Iteration 178: Loss = 460743.2901686755\n",
      "Iteration 179: Loss = 460743.1202523505\n",
      "Iteration 180: Loss = 460742.9551502827\n",
      "Iteration 181: Loss = 460742.794726167\n",
      "Iteration 182: Loss = 460742.6388475304\n",
      "Iteration 183: Loss = 460742.4873856321\n",
      "Iteration 184: Loss = 460742.340215433\n",
      "Iteration 185: Loss = 460742.1972153926\n",
      "Iteration 186: Loss = 460742.0582673812\n",
      "Iteration 187: Loss = 460741.9232566611\n",
      "Iteration 188: Loss = 460741.79207172117\n",
      "Iteration 189: Loss = 460741.66460420605\n",
      "Iteration 190: Loss = 460741.5407488114\n",
      "Iteration 191: Loss = 460741.42040323844\n",
      "Iteration 192: Loss = 460741.3034680602\n",
      "Iteration 193: Loss = 460741.18984670064\n",
      "Iteration 194: Loss = 460741.0794452746\n",
      "Iteration 195: Loss = 460740.97217256547\n",
      "Iteration 196: Loss = 460740.86793994566\n",
      "Iteration 197: Loss = 460740.7666613162\n",
      "Iteration 198: Loss = 460740.668252958\n",
      "Iteration 199: Loss = 460740.57263358386\n",
      "Iteration 200: Loss = 460740.47972418\n",
      "Iteration 201: Loss = 460740.38944796013\n",
      "Iteration 202: Loss = 460740.3017303336\n",
      "Iteration 203: Loss = 460740.2164987982\n",
      "Iteration 204: Loss = 460740.13368294\n",
      "Iteration 205: Loss = 460740.0532142854\n",
      "Iteration 206: Loss = 460739.9750263467\n",
      "Iteration 207: Loss = 460739.89905450656\n",
      "Iteration 208: Loss = 460739.8252359689\n",
      "Iteration 209: Loss = 460739.7535097228\n",
      "Iteration 210: Loss = 460739.68381646514\n",
      "Iteration 211: Loss = 460739.61609860876\n",
      "Iteration 212: Loss = 460739.55030018574\n",
      "Iteration 213: Loss = 460739.4863667878\n",
      "Iteration 214: Loss = 460739.4242455538\n",
      "Iteration 215: Loss = 460739.36388517445\n",
      "Iteration 216: Loss = 460739.30523570324\n",
      "Iteration 217: Loss = 460739.2482486838\n",
      "Iteration 218: Loss = 460739.1928770132\n",
      "Iteration 219: Loss = 460739.13907488005\n",
      "Iteration 220: Loss = 460739.0867978345\n",
      "Iteration 221: Loss = 460739.03600264684\n",
      "Iteration 222: Loss = 460738.98664732423\n",
      "Iteration 223: Loss = 460738.93869104894\n",
      "Iteration 224: Loss = 460738.8920941868\n",
      "Iteration 225: Loss = 460738.8468181889\n",
      "Iteration 226: Loss = 460738.80282564054\n",
      "Iteration 227: Loss = 460738.7600801694\n",
      "Iteration 228: Loss = 460738.7185464116\n",
      "Iteration 229: Loss = 460738.67819003935\n",
      "Iteration 230: Loss = 460738.6389776728\n",
      "Iteration 231: Loss = 460738.6008769041\n",
      "Iteration 232: Loss = 460738.5638562024\n",
      "Iteration 233: Loss = 460738.5278849718\n",
      "Iteration 234: Loss = 460738.49293346534\n",
      "Iteration 235: Loss = 460738.4589727723\n",
      "Iteration 236: Loss = 460738.42597481917\n",
      "Iteration 237: Loss = 460738.3939123068\n",
      "Iteration 238: Loss = 460738.3627587358\n",
      "Iteration 239: Loss = 460738.3324883285\n",
      "Iteration 240: Loss = 460738.30307607213\n",
      "Iteration 241: Loss = 460738.27449761046\n",
      "Iteration 242: Loss = 460738.246729329\n",
      "Iteration 243: Loss = 460738.2197482599\n",
      "Iteration 244: Loss = 460738.1935320877\n",
      "Iteration 245: Loss = 460738.16805913724\n",
      "Iteration 246: Loss = 460738.14330833236\n",
      "Iteration 247: Loss = 460738.1192592048\n",
      "Iteration 248: Loss = 460738.09589187667\n",
      "Iteration 249: Loss = 460738.07318699744\n",
      "Iteration 250: Loss = 460738.051125803\n",
      "Iteration 251: Loss = 460738.0296900445\n",
      "Iteration 252: Loss = 460738.00886199676\n",
      "Iteration 253: Loss = 460737.98862444406\n",
      "Iteration 254: Loss = 460737.968960612\n",
      "Iteration 255: Loss = 460737.9498542637\n",
      "Iteration 256: Loss = 460737.9312895971\n",
      "Iteration 257: Loss = 460737.9132512419\n",
      "Iteration 258: Loss = 460737.8957242919\n",
      "Iteration 259: Loss = 460737.8786942392\n",
      "Iteration 260: Loss = 460737.86214700143\n",
      "Iteration 261: Loss = 460737.84606888815\n",
      "Iteration 262: Loss = 460737.8304466164\n",
      "Iteration 263: Loss = 460737.8152672536\n",
      "Iteration 264: Loss = 460737.8005182325\n",
      "Iteration 265: Loss = 460737.7861873608\n",
      "Iteration 266: Loss = 460737.7722627896\n",
      "Iteration 267: Loss = 460737.75873299304\n",
      "Iteration 268: Loss = 460737.7455867909\n",
      "Iteration 269: Loss = 460737.73281328304\n",
      "Iteration 270: Loss = 460737.72040195053\n",
      "Iteration 271: Loss = 460737.7083424756\n",
      "Iteration 272: Loss = 460737.69662491704\n",
      "Iteration 273: Loss = 460737.68523956934\n",
      "Iteration 274: Loss = 460737.67417700955\n",
      "Iteration 275: Loss = 460737.6634280925\n",
      "Iteration 276: Loss = 460737.6529839233\n",
      "Iteration 277: Loss = 460737.64283586893\n",
      "Iteration 278: Loss = 460737.6329755285\n",
      "Iteration 279: Loss = 460737.62339474447\n",
      "Iteration 280: Loss = 460737.61408560065\n",
      "Iteration 281: Loss = 460737.60504039016\n",
      "Iteration 282: Loss = 460737.5962516081\n",
      "Iteration 283: Loss = 460737.587712021\n",
      "Iteration 284: Loss = 460737.5794145566\n",
      "Iteration 285: Loss = 460737.57135233644\n",
      "Iteration 286: Loss = 460737.5635186905\n",
      "Iteration 287: Loss = 460737.55590715795\n",
      "Iteration 288: Loss = 460737.5485114166\n",
      "Iteration 289: Loss = 460737.5413253632\n",
      "Iteration 290: Loss = 460737.534343045\n",
      "Iteration 291: Loss = 460737.52755870885\n",
      "Iteration 292: Loss = 460737.5209667199\n",
      "Iteration 293: Loss = 460737.5145616218\n",
      "Iteration 294: Loss = 460737.5083381201\n",
      "Iteration 295: Loss = 460737.50229108456\n",
      "Iteration 296: Loss = 460737.4964154699\n",
      "Iteration 297: Loss = 460737.49070647586\n",
      "Iteration 298: Loss = 460737.4851593368\n",
      "Iteration 299: Loss = 460737.4797694665\n",
      "Iteration 300: Loss = 460737.47453240893\n",
      "Iteration 301: Loss = 460737.46944383555\n",
      "Iteration 302: Loss = 460737.4644995551\n",
      "Iteration 303: Loss = 460737.45969542814\n",
      "Iteration 304: Loss = 460737.4550275351\n",
      "Iteration 305: Loss = 460737.4504919771\n",
      "Iteration 306: Loss = 460737.4460850317\n",
      "Iteration 307: Loss = 460737.44180302013\n",
      "Iteration 308: Loss = 460737.4376424126\n",
      "Iteration 309: Loss = 460737.43359977874\n",
      "Iteration 310: Loss = 460737.4296717561\n",
      "Iteration 311: Loss = 460737.425855121\n",
      "Iteration 312: Loss = 460737.42214667087\n",
      "Iteration 313: Loss = 460737.4185433811\n",
      "Iteration 314: Loss = 460737.4150422581\n",
      "Iteration 315: Loss = 460737.41164039867\n",
      "Iteration 316: Loss = 460737.4083349867\n",
      "Iteration 317: Loss = 460737.4051232998\n",
      "Iteration 318: Loss = 460737.40200267313\n",
      "Iteration 319: Loss = 460737.3989705245\n",
      "Iteration 320: Loss = 460737.3960243412\n",
      "Iteration 321: Loss = 460737.3931616983\n",
      "Iteration 322: Loss = 460737.3903802069\n",
      "Iteration 323: Loss = 460737.3876775915\n",
      "Iteration 324: Loss = 460737.385051594\n",
      "Iteration 325: Loss = 460737.38250006735\n",
      "Iteration 326: Loss = 460737.38002087246\n",
      "Iteration 327: Loss = 460737.3776119732\n",
      "Iteration 328: Loss = 460737.37527137314\n",
      "Iteration 329: Loss = 460737.37299712806\n",
      "Iteration 330: Loss = 460737.37078737235\n",
      "Iteration 331: Loss = 460737.36864026746\n",
      "Iteration 332: Loss = 460737.36655406107\n",
      "Iteration 333: Loss = 460737.3645269772\n",
      "Iteration 334: Loss = 460737.362557376\n",
      "Iteration 335: Loss = 460737.36064361763\n",
      "Iteration 336: Loss = 460737.35878411133\n",
      "Iteration 337: Loss = 460737.3569773587\n",
      "Iteration 338: Loss = 460737.35522181\n",
      "Iteration 339: Loss = 460737.35351603746\n",
      "Iteration 340: Loss = 460737.35185863817\n",
      "Iteration 341: Loss = 460737.3502482188\n",
      "Iteration 342: Loss = 460737.3486834576\n",
      "Iteration 343: Loss = 460737.3471630877\n",
      "Iteration 344: Loss = 460737.34568579943\n",
      "Iteration 345: Loss = 460737.34425041464\n",
      "Iteration 346: Loss = 460737.34285571665\n",
      "Iteration 347: Loss = 460737.3415005524\n",
      "Iteration 348: Loss = 460737.34018383187\n",
      "Iteration 349: Loss = 460737.3389044325\n",
      "Iteration 350: Loss = 460737.33766131435\n",
      "Iteration 351: Loss = 460737.3364534406\n",
      "Iteration 352: Loss = 460737.33527982753\n",
      "Iteration 353: Loss = 460737.3341394766\n",
      "Iteration 354: Loss = 460737.3330314539\n",
      "Iteration 355: Loss = 460737.3319548553\n",
      "Iteration 356: Loss = 460737.3309087726\n",
      "Iteration 357: Loss = 460737.3298923642\n",
      "Iteration 358: Loss = 460737.32890476775\n",
      "Iteration 359: Loss = 460737.32794516394\n",
      "Iteration 360: Loss = 460737.3270127724\n",
      "Iteration 361: Loss = 460737.326106826\n",
      "Iteration 362: Loss = 460737.32522656384\n",
      "Iteration 363: Loss = 460737.3243712581\n",
      "Iteration 364: Loss = 460737.3235402047\n",
      "Iteration 365: Loss = 460737.32273271435\n",
      "Iteration 366: Loss = 460737.32194811566\n",
      "Iteration 367: Loss = 460737.32118575653\n",
      "Iteration 368: Loss = 460737.3204450159\n",
      "Iteration 369: Loss = 460737.3197252956\n",
      "Iteration 370: Loss = 460737.31902596744\n",
      "Iteration 371: Loss = 460737.3183464581\n",
      "Iteration 372: Loss = 460737.317686227\n",
      "Iteration 373: Loss = 460737.31704471365\n",
      "Iteration 374: Loss = 460737.31642140175\n",
      "Iteration 375: Loss = 460737.3158157409\n",
      "Iteration 376: Loss = 460737.31522725685\n",
      "Iteration 377: Loss = 460737.31465546804\n",
      "Iteration 378: Loss = 460737.31409989676\n",
      "Iteration 379: Loss = 460737.3135600751\n",
      "Iteration 380: Loss = 460737.31303554936\n",
      "Iteration 381: Loss = 460737.3125258889\n",
      "Iteration 382: Loss = 460737.3120306998\n",
      "Iteration 383: Loss = 460737.31154953653\n",
      "Iteration 384: Loss = 460737.31108201155\n",
      "Iteration 385: Loss = 460737.31062776217\n",
      "Iteration 386: Loss = 460737.31018638273\n",
      "Iteration 387: Loss = 460737.3097574988\n",
      "Iteration 388: Loss = 460737.3093408053\n",
      "Iteration 389: Loss = 460737.30893591867\n",
      "Iteration 390: Loss = 460737.30854250473\n",
      "Iteration 391: Loss = 460737.3081602476\n",
      "Iteration 392: Loss = 460737.30778882524\n",
      "Iteration 393: Loss = 460737.30742794636\n",
      "Iteration 394: Loss = 460737.30707728775\n",
      "Iteration 395: Loss = 460737.30673657847\n",
      "Iteration 396: Loss = 460737.30640552513\n",
      "Iteration 397: Loss = 460737.30608385184\n",
      "Iteration 398: Loss = 460737.30577131367\n",
      "Iteration 399: Loss = 460737.3054676347\n",
      "Iteration 400: Loss = 460737.30517255055\n",
      "Iteration 401: Loss = 460737.3048858353\n",
      "Iteration 402: Loss = 460737.3046072624\n",
      "Iteration 403: Loss = 460737.3043365953\n",
      "Iteration 404: Loss = 460737.30407358834\n",
      "Iteration 405: Loss = 460737.3038180348\n",
      "Iteration 406: Loss = 460737.30356973637\n",
      "Iteration 407: Loss = 460737.3033284726\n",
      "Iteration 408: Loss = 460737.30309405696\n",
      "Iteration 409: Loss = 460737.3028662774\n",
      "Iteration 410: Loss = 460737.3026449683\n",
      "Iteration 411: Loss = 460737.30242992146\n",
      "Iteration 412: Loss = 460737.30222097976\n",
      "Iteration 413: Loss = 460737.302017968\n",
      "Iteration 414: Loss = 460737.3018206919\n",
      "Iteration 415: Loss = 460737.3016290192\n",
      "Iteration 416: Loss = 460737.3014427914\n",
      "Iteration 417: Loss = 460737.3012618309\n",
      "Iteration 418: Loss = 460737.30108600674\n",
      "Iteration 419: Loss = 460737.30091517675\n",
      "Iteration 420: Loss = 460737.300749186\n",
      "Iteration 421: Loss = 460737.300587891\n",
      "Iteration 422: Loss = 460737.3004311753\n",
      "Iteration 423: Loss = 460737.3002788937\n",
      "Iteration 424: Loss = 460737.3001309492\n",
      "Iteration 425: Loss = 460737.2999871838\n",
      "Iteration 426: Loss = 460737.2998475003\n",
      "Iteration 427: Loss = 460737.29971177946\n",
      "Iteration 428: Loss = 460737.2995799029\n",
      "Iteration 429: Loss = 460737.2994517806\n",
      "Iteration 430: Loss = 460737.2993272577\n",
      "Iteration 431: Loss = 460737.29920629313\n",
      "Iteration 432: Loss = 460737.2990887578\n",
      "Iteration 433: Loss = 460737.2989745412\n",
      "Iteration 434: Loss = 460737.29886356345\n",
      "Iteration 435: Loss = 460737.2987557482\n",
      "Iteration 436: Loss = 460737.2986509861\n",
      "Iteration 437: Loss = 460737.298549172\n",
      "Iteration 438: Loss = 460737.2984502692\n",
      "Iteration 439: Loss = 460737.2983541681\n",
      "Iteration 440: Loss = 460737.29826078296\n",
      "Iteration 441: Loss = 460737.2981700463\n",
      "Iteration 442: Loss = 460737.29808188725\n",
      "Iteration 443: Loss = 460737.2979962231\n",
      "Iteration 444: Loss = 460737.29791298683\n",
      "Iteration 445: Loss = 460737.29783211823\n",
      "Iteration 446: Loss = 460737.2977535341\n",
      "Iteration 447: Loss = 460737.297677188\n",
      "Iteration 448: Loss = 460737.2976029973\n",
      "Iteration 449: Loss = 460737.2975309115\n",
      "Iteration 450: Loss = 460737.2974608812\n",
      "Iteration 451: Loss = 460737.2973928239\n",
      "Iteration 452: Loss = 460737.2973266971\n",
      "Iteration 453: Loss = 460737.297262455\n",
      "Iteration 454: Loss = 460737.2972000295\n",
      "Iteration 455: Loss = 460737.2971393617\n",
      "Iteration 456: Loss = 460737.29708042444\n",
      "Iteration 457: Loss = 460737.29702315474\n",
      "Iteration 458: Loss = 460737.29696751287\n",
      "Iteration 459: Loss = 460737.29691345186\n",
      "Iteration 460: Loss = 460737.2968609232\n",
      "Iteration 461: Loss = 460737.29680988035\n",
      "Iteration 462: Loss = 460737.2967602818\n",
      "Iteration 463: Loss = 460737.29671208264\n",
      "Iteration 464: Loss = 460737.2966652685\n",
      "Iteration 465: Loss = 460737.29661977023\n",
      "Iteration 466: Loss = 460737.2965755712\n",
      "Iteration 467: Loss = 460737.29653261346\n",
      "Iteration 468: Loss = 460737.2964908838\n",
      "Iteration 469: Loss = 460737.29645032895\n",
      "Iteration 470: Loss = 460737.2964109253\n",
      "Iteration 471: Loss = 460737.2963726443\n",
      "Iteration 472: Loss = 460737.2963354534\n",
      "Iteration 473: Loss = 460737.2962992918\n",
      "Iteration 474: Loss = 460737.2962641834\n",
      "Iteration 475: Loss = 460737.2962300604\n",
      "Iteration 476: Loss = 460737.296196907\n",
      "Iteration 477: Loss = 460737.2961646908\n",
      "Iteration 478: Loss = 460737.29613338754\n",
      "Iteration 479: Loss = 460737.2961029777\n",
      "Iteration 480: Loss = 460737.29607342184\n",
      "Iteration 481: Loss = 460737.2960447032\n",
      "Iteration 482: Loss = 460737.29601680185\n",
      "Iteration 483: Loss = 460737.295989692\n",
      "Iteration 484: Loss = 460737.29596334905\n",
      "Iteration 485: Loss = 460737.2959377638\n",
      "Iteration 486: Loss = 460737.2959128857\n",
      "Iteration 487: Loss = 460737.2958887274\n",
      "Iteration 488: Loss = 460737.29586525384\n",
      "Iteration 489: Loss = 460737.29584244016\n",
      "Iteration 490: Loss = 460737.2958202773\n",
      "Iteration 491: Loss = 460737.2957987364\n",
      "Iteration 492: Loss = 460737.29577780876\n",
      "Iteration 493: Loss = 460737.29575748043\n",
      "Iteration 494: Loss = 460737.2957377232\n",
      "Iteration 495: Loss = 460737.2957185354\n",
      "Iteration 496: Loss = 460737.29569987644\n",
      "Iteration 497: Loss = 460737.29568174906\n",
      "Iteration 498: Loss = 460737.2956641366\n",
      "Iteration 499: Loss = 460737.29564702587\n",
      "Iteration 500: Loss = 460737.2956304062\n",
      "Iteration 501: Loss = 460737.2956142477\n",
      "Iteration 502: Loss = 460737.29559855745\n",
      "Iteration 503: Loss = 460737.29558330023\n",
      "Iteration 504: Loss = 460737.2955684921\n",
      "Iteration 505: Loss = 460737.2955540889\n",
      "Iteration 506: Loss = 460737.2955400996\n",
      "Iteration 507: Loss = 460737.2955265115\n",
      "Iteration 508: Loss = 460737.2955132993\n",
      "Iteration 509: Loss = 460737.2955004632\n",
      "Iteration 510: Loss = 460737.29548799596\n",
      "Iteration 511: Loss = 460737.29547588056\n",
      "Iteration 512: Loss = 460737.29546411213\n",
      "Iteration 513: Loss = 460737.2954526708\n",
      "Iteration 514: Loss = 460737.2954415624\n",
      "Iteration 515: Loss = 460737.29543075274\n",
      "Iteration 516: Loss = 460737.29542027105\n",
      "Iteration 517: Loss = 460737.2954100775\n",
      "Iteration 518: Loss = 460737.29540016246\n"
     ]
    }
   ],
   "source": [
    "# Coordinate Descent\n",
    "\n",
    "objLog = {}\n",
    "lastObjective = None\n",
    "bestObjective = None\n",
    "iterations = 1000\n",
    "tol = 1e-5\n",
    "lamb = 1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    alpha_ = iterate(lamb)\n",
    "    currObjective = calculate_objective(alpha_, lamb)\n",
    "    print(f'Iteration {iter+1}: Loss = {currObjective}')\n",
    "    if lastObjective and abs(lastObjective-currObjective) < tol:\n",
    "        bestObjective = currObjective\n",
    "        break\n",
    "    lastObjective = currObjective\n",
    "    objLog[iter+1] = currObjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE q6\n",
    "\n",
    "actual_q6 = [h[2]['hours_transformed'] for h in hoursValid]\n",
    "preds_q6 = []\n",
    "\n",
    "for h in hoursValid:\n",
    "    user, game = h[0], h[1]\n",
    "    pred = alpha_ + betaU[user] + betaI[game]\n",
    "    preds_q6.append(pred)\n",
    "\n",
    "validMSE = MSE(np.array(actual_q6), np.array(preds_q6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6534a08d-013e-4353-a12c-b1f2bbed5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bc0e3695-682b-4d65-9576-c59795d04930",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684,\n",
       " 'Q2': [96249.45000000001, 0.6954195419541954],\n",
       " 'Q3': 0.673067306730673,\n",
       " 'Q4': 0.6867186718671867,\n",
       " 'Q5': 'I confirm that I have uploaded an assignment submission to gradescope',\n",
       " 'Q6': 3.0073728449436676,\n",
       " 'Q7': [6.475711427058052,\n",
       "  -3.4530536681732302,\n",
       "  5.060566989683245,\n",
       "  -3.4578940441101294],\n",
       " 'Q8': (1.3, 5.316020858071471)}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d9d419e4-e8c4-4766-b189-d77fbe608417",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4a48cc70-1c2c-40df-9843-fea1f287a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum betaU = u60898505 (5.828516272137527)\n",
      "Maximum betaI = g17604638 (5.513882931578414)\n",
      "Minimum betaU = u13037838 (-3.0057164456685355)\n",
      "Minimum betaI = g84397720 (-2.791543895327124)\n"
     ]
    }
   ],
   "source": [
    "betaUs = [(betaU[u], u) for u in betaU]\n",
    "betaIs = [(betaI[i], i) for i in betaI]\n",
    "betaUs.sort()\n",
    "betaIs.sort()\n",
    "\n",
    "print(\"Maximum betaU = \" + str(betaUs[-1][1]) + ' (' + str(betaUs[-1][0]) + ')')\n",
    "print(\"Maximum betaI = \" + str(betaIs[-1][1]) + ' (' + str(betaIs[-1][0]) + ')')\n",
    "print(\"Minimum betaU = \" + str(betaUs[0][1]) + ' (' + str(betaUs[0][0]) + ')')\n",
    "print(\"Minimum betaI = \" + str(betaIs[0][1]) + ' (' + str(betaIs[0][0]) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "65b17529-ade3-4cdf-a5c1-b17b06e68237",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [betaUs[-1][0], betaUs[0][0], betaIs[-1][0], betaIs[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4eeaf180-3bd8-4acb-aef5-86b044521e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.828516272137527, -3.0057164456685355, 5.513882931578414, -2.791543895327124]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7c9faa5c-2bc1-4d51-ae29-df2d82c9372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684,\n",
       " 'Q2': [96249.45000000001, 0.6954195419541954],\n",
       " 'Q3': 0.673067306730673,\n",
       " 'Q4': 0.6867186718671867,\n",
       " 'Q5': 'I confirm that I have uploaded an assignment submission to gradescope',\n",
       " 'Q6': 3.0073728449436676,\n",
       " 'Q7': [5.828516272137527,\n",
       "  -3.0057164456685355,\n",
       "  5.513882931578414,\n",
       "  -2.791543895327124],\n",
       " 'Q8': (1.3, 5.316020858071471)}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c604fd19-2fb8-44bf-82b5-33797f534707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 492012.7909083451\n",
      "Iteration 2: Loss = 459708.0365521542\n",
      "Iteration 3: Loss = 458187.36394561816\n",
      "Iteration 4: Loss = 458074.88363084715\n",
      "Iteration 5: Loss = 458060.48007246025\n",
      "Iteration 6: Loss = 458053.465785643\n",
      "Iteration 7: Loss = 458047.0920641419\n",
      "Iteration 8: Loss = 458040.84688122186\n",
      "Iteration 9: Loss = 458034.69019834924\n",
      "Iteration 10: Loss = 458028.6179332313\n",
      "Iteration 11: Loss = 458022.6287656636\n",
      "Iteration 12: Loss = 458016.7216002457\n",
      "Iteration 13: Loss = 458010.89537081274\n",
      "Iteration 14: Loss = 458005.14902497234\n",
      "Iteration 15: Loss = 457999.48152258305\n",
      "Iteration 16: Loss = 457993.8918356114\n",
      "Iteration 17: Loss = 457988.3789479926\n",
      "Iteration 18: Loss = 457982.9418554739\n",
      "Iteration 19: Loss = 457977.57956560317\n",
      "Iteration 20: Loss = 457972.2910975604\n",
      "Iteration 21: Loss = 457967.0754820387\n",
      "Iteration 22: Loss = 457961.9317612602\n",
      "Iteration 23: Loss = 457956.858988755\n",
      "Iteration 24: Loss = 457951.85622930503\n",
      "Iteration 25: Loss = 457946.92255887116\n",
      "Iteration 26: Loss = 457942.0570644288\n",
      "Iteration 27: Loss = 457937.25884395244\n",
      "Iteration 28: Loss = 457932.5270062306\n",
      "Iteration 29: Loss = 457927.8606708401\n",
      "Iteration 30: Loss = 457923.25896797125\n",
      "Iteration 31: Loss = 457918.72103836585\n",
      "Iteration 32: Loss = 457914.2460332559\n",
      "Iteration 33: Loss = 457909.83311419375\n",
      "Iteration 34: Loss = 457905.481452976\n",
      "Iteration 35: Loss = 457901.19023155124\n",
      "Iteration 36: Loss = 457896.95864195924\n",
      "Iteration 37: Loss = 457892.7858861328\n",
      "Iteration 38: Loss = 457888.6711758852\n",
      "Iteration 39: Loss = 457884.6137327936\n",
      "Iteration 40: Loss = 457880.61278805113\n",
      "Iteration 41: Loss = 457876.66758247185\n",
      "Iteration 42: Loss = 457872.77736625273\n",
      "Iteration 43: Loss = 457868.94139903306\n",
      "Iteration 44: Loss = 457865.1589496337\n",
      "Iteration 45: Loss = 457861.4292961123\n",
      "Iteration 46: Loss = 457857.7517255595\n",
      "Iteration 47: Loss = 457854.12553406775\n",
      "Iteration 48: Loss = 457850.55002656905\n",
      "Iteration 49: Loss = 457847.024516833\n",
      "Iteration 50: Loss = 457843.54832731734\n",
      "Iteration 51: Loss = 457840.1207890287\n",
      "Iteration 52: Loss = 457836.7412415255\n",
      "Iteration 53: Loss = 457833.4090327698\n",
      "Iteration 54: Loss = 457830.1235190279\n",
      "Iteration 55: Loss = 457826.8840648209\n",
      "Iteration 56: Loss = 457823.6900428073\n",
      "Iteration 57: Loss = 457820.5408336707\n",
      "Iteration 58: Loss = 457817.4358260793\n",
      "Iteration 59: Loss = 457814.37441653863\n",
      "Iteration 60: Loss = 457811.35600938735\n",
      "Iteration 61: Loss = 457808.3800166077\n",
      "Iteration 62: Loss = 457805.44585781597\n",
      "Iteration 63: Loss = 457802.55296014465\n",
      "Iteration 64: Loss = 457799.7007581574\n",
      "Iteration 65: Loss = 457796.8886937505\n",
      "Iteration 66: Loss = 457794.1162161478\n",
      "Iteration 67: Loss = 457791.3827816543\n",
      "Iteration 68: Loss = 457788.68785376265\n",
      "Iteration 69: Loss = 457786.03090292204\n",
      "Iteration 70: Loss = 457783.4114065642\n",
      "Iteration 71: Loss = 457780.82884892024\n",
      "Iteration 72: Loss = 457778.28272101085\n",
      "Iteration 73: Loss = 457775.77252057556\n",
      "Iteration 74: Loss = 457773.297751957\n",
      "Iteration 75: Loss = 457770.85792598175\n",
      "Iteration 76: Loss = 457768.4525599843\n",
      "Iteration 77: Loss = 457766.08117767115\n",
      "Iteration 78: Loss = 457763.7433090437\n",
      "Iteration 79: Loss = 457761.4384903131\n",
      "Iteration 80: Loss = 457759.1662638799\n",
      "Iteration 81: Loss = 457756.9261781748\n",
      "Iteration 82: Loss = 457754.71778768697\n",
      "Iteration 83: Loss = 457752.54065280734\n",
      "Iteration 84: Loss = 457750.3943397436\n",
      "Iteration 85: Loss = 457748.27842057985\n",
      "Iteration 86: Loss = 457746.1924730465\n",
      "Iteration 87: Loss = 457744.1360805486\n",
      "Iteration 88: Loss = 457742.10883205844\n",
      "Iteration 89: Loss = 457740.1103220616\n",
      "Iteration 90: Loss = 457738.1401504683\n",
      "Iteration 91: Loss = 457736.19792257814\n",
      "Iteration 92: Loss = 457734.28324899275\n",
      "Iteration 93: Loss = 457732.39574555564\n",
      "Iteration 94: Loss = 457730.5350332715\n",
      "Iteration 95: Loss = 457728.70073827\n",
      "Iteration 96: Loss = 457726.89249170804\n",
      "Iteration 97: Loss = 457725.1099297223\n",
      "Iteration 98: Loss = 457723.3526934131\n",
      "Iteration 99: Loss = 457721.62042869313\n",
      "Iteration 100: Loss = 457719.9127862566\n",
      "Iteration 101: Loss = 457718.2294215755\n",
      "Iteration 102: Loss = 457716.56999478716\n",
      "Iteration 103: Loss = 457714.9341706377\n",
      "Iteration 104: Loss = 457713.32161839743\n",
      "Iteration 105: Loss = 457711.73201189947\n",
      "Iteration 106: Loss = 457710.1650293787\n",
      "Iteration 107: Loss = 457708.620353448\n",
      "Iteration 108: Loss = 457707.09767105227\n",
      "Iteration 109: Loss = 457705.59667343623\n",
      "Iteration 110: Loss = 457704.1170560338\n",
      "Iteration 111: Loss = 457702.65851843706\n",
      "Iteration 112: Loss = 457701.2207643634\n",
      "Iteration 113: Loss = 457699.8035015942\n",
      "Iteration 114: Loss = 457698.4064418767\n",
      "Iteration 115: Loss = 457697.02930095553\n",
      "Iteration 116: Loss = 457695.67179844726\n",
      "Iteration 117: Loss = 457694.3336578183\n",
      "Iteration 118: Loss = 457693.0146063417\n",
      "Iteration 119: Loss = 457691.71437503403\n",
      "Iteration 120: Loss = 457690.4326986134\n",
      "Iteration 121: Loss = 457689.1693154588\n",
      "Iteration 122: Loss = 457687.92396753904\n",
      "Iteration 123: Loss = 457686.69640039856\n",
      "Iteration 124: Loss = 457685.4863630488\n",
      "Iteration 125: Loss = 457684.2936080366\n",
      "Iteration 126: Loss = 457683.1178912611\n",
      "Iteration 127: Loss = 457681.95897201274\n",
      "Iteration 128: Loss = 457680.8166129289\n",
      "Iteration 129: Loss = 457679.6905799116\n",
      "Iteration 130: Loss = 457678.5806421053\n",
      "Iteration 131: Loss = 457677.48657185124\n",
      "Iteration 132: Loss = 457676.4081446989\n",
      "Iteration 133: Loss = 457675.3451391878\n",
      "Iteration 134: Loss = 457674.29733704665\n",
      "Iteration 135: Loss = 457673.26452299865\n",
      "Iteration 136: Loss = 457672.24648473394\n",
      "Iteration 137: Loss = 457671.2430129093\n",
      "Iteration 138: Loss = 457670.25390111003\n",
      "Iteration 139: Loss = 457669.2789457427\n",
      "Iteration 140: Loss = 457668.3179461169\n",
      "Iteration 141: Loss = 457667.37070428947\n",
      "Iteration 142: Loss = 457666.4370250754\n",
      "Iteration 143: Loss = 457665.5167160415\n",
      "Iteration 144: Loss = 457664.6095873998\n",
      "Iteration 145: Loss = 457663.7154520427\n",
      "Iteration 146: Loss = 457662.8341254476\n",
      "Iteration 147: Loss = 457661.9654257104\n",
      "Iteration 148: Loss = 457661.10917341564\n",
      "Iteration 149: Loss = 457660.26519168576\n",
      "Iteration 150: Loss = 457659.4333061101\n",
      "Iteration 151: Loss = 457658.6133447485\n",
      "Iteration 152: Loss = 457657.80513799196\n",
      "Iteration 153: Loss = 457657.00851869455\n",
      "Iteration 154: Loss = 457656.22332200804\n",
      "Iteration 155: Loss = 457655.449385375\n",
      "Iteration 156: Loss = 457654.68654856744\n",
      "Iteration 157: Loss = 457653.9346535659\n",
      "Iteration 158: Loss = 457653.1935445651\n",
      "Iteration 159: Loss = 457652.46306799393\n",
      "Iteration 160: Loss = 457651.7430723999\n",
      "Iteration 161: Loss = 457651.0334084403\n",
      "Iteration 162: Loss = 457650.33392892836\n",
      "Iteration 163: Loss = 457649.6444886853\n",
      "Iteration 164: Loss = 457648.96494463255\n",
      "Iteration 165: Loss = 457648.2951556584\n",
      "Iteration 166: Loss = 457647.63498264883\n",
      "Iteration 167: Loss = 457646.9842884686\n",
      "Iteration 168: Loss = 457646.34293789626\n",
      "Iteration 169: Loss = 457645.7107976099\n",
      "Iteration 170: Loss = 457645.0877361999\n",
      "Iteration 171: Loss = 457644.4736240583\n",
      "Iteration 172: Loss = 457643.8683334415\n",
      "Iteration 173: Loss = 457643.2717384111\n",
      "Iteration 174: Loss = 457642.6837147752\n",
      "Iteration 175: Loss = 457642.1041401263\n",
      "Iteration 176: Loss = 457641.5328937672\n",
      "Iteration 177: Loss = 457640.9698567095\n",
      "Iteration 178: Loss = 457640.41491166607\n",
      "Iteration 179: Loss = 457639.86794296524\n",
      "Iteration 180: Loss = 457639.32883662015\n",
      "Iteration 181: Loss = 457638.79748020513\n",
      "Iteration 182: Loss = 457638.2737629394\n",
      "Iteration 183: Loss = 457637.75757556804\n",
      "Iteration 184: Loss = 457637.24881040066\n",
      "Iteration 185: Loss = 457636.74736126454\n",
      "Iteration 186: Loss = 457636.2531235048\n",
      "Iteration 187: Loss = 457635.7659939094\n",
      "Iteration 188: Loss = 457635.28587080317\n",
      "Iteration 189: Loss = 457634.81265386904\n",
      "Iteration 190: Loss = 457634.3462442749\n",
      "Iteration 191: Loss = 457633.8865445415\n",
      "Iteration 192: Loss = 457633.4334586292\n",
      "Iteration 193: Loss = 457632.9868917966\n",
      "Iteration 194: Loss = 457632.54675070004\n",
      "Iteration 195: Loss = 457632.1129432945\n",
      "Iteration 196: Loss = 457631.6853788594\n",
      "Iteration 197: Loss = 457631.26396792196\n",
      "Iteration 198: Loss = 457630.84862234787\n",
      "Iteration 199: Loss = 457630.439255182\n",
      "Iteration 200: Loss = 457630.0357807452\n",
      "Iteration 201: Loss = 457629.63811460807\n",
      "Iteration 202: Loss = 457629.2461734593\n",
      "Iteration 203: Loss = 457628.85987525777\n",
      "Iteration 204: Loss = 457628.4791390438\n",
      "Iteration 205: Loss = 457628.1038850942\n",
      "Iteration 206: Loss = 457627.7340347657\n",
      "Iteration 207: Loss = 457627.3695105582\n",
      "Iteration 208: Loss = 457627.0102360585\n",
      "Iteration 209: Loss = 457626.6561359577\n",
      "Iteration 210: Loss = 457626.3071360207\n",
      "Iteration 211: Loss = 457625.96316304244\n",
      "Iteration 212: Loss = 457625.624144891\n",
      "Iteration 213: Loss = 457625.29001046286\n",
      "Iteration 214: Loss = 457624.96068963996\n",
      "Iteration 215: Loss = 457624.63611332094\n",
      "Iteration 216: Loss = 457624.31621339853\n",
      "Iteration 217: Loss = 457624.00092272775\n",
      "Iteration 218: Loss = 457623.6901751185\n",
      "Iteration 219: Loss = 457623.3839053387\n",
      "Iteration 220: Loss = 457623.08204905025\n",
      "Iteration 221: Loss = 457622.7845428954\n",
      "Iteration 222: Loss = 457622.4913243647\n",
      "Iteration 223: Loss = 457622.2023318799\n",
      "Iteration 224: Loss = 457621.9175046893\n",
      "Iteration 225: Loss = 457621.63678297785\n",
      "Iteration 226: Loss = 457621.36010772473\n",
      "Iteration 227: Loss = 457621.08742079884\n",
      "Iteration 228: Loss = 457620.81866487494\n",
      "Iteration 229: Loss = 457620.55378345575\n",
      "Iteration 230: Loss = 457620.29272083414\n",
      "Iteration 231: Loss = 457620.0354221311\n",
      "Iteration 232: Loss = 457619.78183321405\n",
      "Iteration 233: Loss = 457619.53190073546\n",
      "Iteration 234: Loss = 457619.285572154\n",
      "Iteration 235: Loss = 457619.04279562784\n",
      "Iteration 236: Loss = 457618.8035200684\n",
      "Iteration 237: Loss = 457618.56769513333\n",
      "Iteration 238: Loss = 457618.3352711805\n",
      "Iteration 239: Loss = 457618.1061993196\n",
      "Iteration 240: Loss = 457617.88043128676\n",
      "Iteration 241: Loss = 457617.65791957756\n",
      "Iteration 242: Loss = 457617.43861733685\n",
      "Iteration 243: Loss = 457617.22247837955\n",
      "Iteration 244: Loss = 457617.00945721177\n",
      "Iteration 245: Loss = 457616.79950895207\n",
      "Iteration 246: Loss = 457616.5925893543\n",
      "Iteration 247: Loss = 457616.38865486806\n",
      "Iteration 248: Loss = 457616.18766251724\n",
      "Iteration 249: Loss = 457615.9895699436\n",
      "Iteration 250: Loss = 457615.7943354069\n",
      "Iteration 251: Loss = 457615.6019177728\n",
      "Iteration 252: Loss = 457615.4122764742\n",
      "Iteration 253: Loss = 457615.2253715447\n",
      "Iteration 254: Loss = 457615.04116359615\n",
      "Iteration 255: Loss = 457614.85961378587\n",
      "Iteration 256: Loss = 457614.6806838469\n",
      "Iteration 257: Loss = 457614.50433601154\n",
      "Iteration 258: Loss = 457614.3305331408\n",
      "Iteration 259: Loss = 457614.1592385545\n",
      "Iteration 260: Loss = 457613.9904161348\n",
      "Iteration 261: Loss = 457613.8240302847\n",
      "Iteration 262: Loss = 457613.6600458788\n",
      "Iteration 263: Loss = 457613.4984283381\n",
      "Iteration 264: Loss = 457613.3391435648\n",
      "Iteration 265: Loss = 457613.1821579386\n",
      "Iteration 266: Loss = 457613.0274383571\n",
      "Iteration 267: Loss = 457612.8749521557\n",
      "Iteration 268: Loss = 457612.7246671485\n",
      "Iteration 269: Loss = 457612.5765516196\n",
      "Iteration 270: Loss = 457612.4305743196\n",
      "Iteration 271: Loss = 457612.28670442314\n",
      "Iteration 272: Loss = 457612.14491155045\n",
      "Iteration 273: Loss = 457612.0051657668\n",
      "Iteration 274: Loss = 457611.86743759137\n",
      "Iteration 275: Loss = 457611.73169793264\n",
      "Iteration 276: Loss = 457611.5979181006\n",
      "Iteration 277: Loss = 457611.4660698629\n",
      "Iteration 278: Loss = 457611.33612537273\n",
      "Iteration 279: Loss = 457611.208057193\n",
      "Iteration 280: Loss = 457611.08183826786\n",
      "Iteration 281: Loss = 457610.9574419368\n",
      "Iteration 282: Loss = 457610.83484190074\n",
      "Iteration 283: Loss = 457610.7140122865\n",
      "Iteration 284: Loss = 457610.5949275487\n",
      "Iteration 285: Loss = 457610.4775625316\n",
      "Iteration 286: Loss = 457610.36189244274\n",
      "Iteration 287: Loss = 457610.24789284065\n",
      "Iteration 288: Loss = 457610.1355396006\n",
      "Iteration 289: Loss = 457610.0248090212\n",
      "Iteration 290: Loss = 457609.91567768273\n",
      "Iteration 291: Loss = 457609.8081225154\n",
      "Iteration 292: Loss = 457609.7021207845\n",
      "Iteration 293: Loss = 457609.59765008645\n",
      "Iteration 294: Loss = 457609.4946883415\n",
      "Iteration 295: Loss = 457609.3932137591\n",
      "Iteration 296: Loss = 457609.293204917\n",
      "Iteration 297: Loss = 457609.1946406523\n",
      "Iteration 298: Loss = 457609.0975001178\n",
      "Iteration 299: Loss = 457609.0017628057\n",
      "Iteration 300: Loss = 457608.90740841965\n",
      "Iteration 301: Loss = 457608.8144170419\n",
      "Iteration 302: Loss = 457608.72276900616\n",
      "Iteration 303: Loss = 457608.6324449163\n",
      "Iteration 304: Loss = 457608.543425667\n",
      "Iteration 305: Loss = 457608.45569244446\n",
      "Iteration 306: Loss = 457608.3692266666\n",
      "Iteration 307: Loss = 457608.28401006403\n",
      "Iteration 308: Loss = 457608.20002460846\n",
      "Iteration 309: Loss = 457608.1172525101\n",
      "Iteration 310: Loss = 457608.0356762885\n",
      "Iteration 311: Loss = 457607.9552786574\n",
      "Iteration 312: Loss = 457607.8760426232\n",
      "Iteration 313: Loss = 457607.7979514227\n",
      "Iteration 314: Loss = 457607.72098851466\n",
      "Iteration 315: Loss = 457607.64513761475\n",
      "Iteration 316: Loss = 457607.57038268115\n",
      "Iteration 317: Loss = 457607.49670789985\n",
      "Iteration 318: Loss = 457607.4240976611\n",
      "Iteration 319: Loss = 457607.3525365947\n",
      "Iteration 320: Loss = 457607.2820095815\n",
      "Iteration 321: Loss = 457607.21250167507\n",
      "Iteration 322: Loss = 457607.1439981597\n",
      "Iteration 323: Loss = 457607.0764845294\n",
      "Iteration 324: Loss = 457607.0099465121\n",
      "Iteration 325: Loss = 457606.9443700239\n",
      "Iteration 326: Loss = 457606.8797411503\n",
      "Iteration 327: Loss = 457606.8160462377\n",
      "Iteration 328: Loss = 457606.7532717926\n",
      "Iteration 329: Loss = 457606.69140453404\n",
      "Iteration 330: Loss = 457606.63043133594\n",
      "Iteration 331: Loss = 457606.57033930277\n",
      "Iteration 332: Loss = 457606.5111157197\n",
      "Iteration 333: Loss = 457606.45274802996\n",
      "Iteration 334: Loss = 457606.3952238936\n",
      "Iteration 335: Loss = 457606.3385311083\n",
      "Iteration 336: Loss = 457606.28265768255\n",
      "Iteration 337: Loss = 457606.2275917809\n",
      "Iteration 338: Loss = 457606.173321717\n",
      "Iteration 339: Loss = 457606.11983602936\n",
      "Iteration 340: Loss = 457606.0671233836\n",
      "Iteration 341: Loss = 457606.0151726087\n",
      "Iteration 342: Loss = 457605.96397267986\n",
      "Iteration 343: Loss = 457605.9135128017\n",
      "Iteration 344: Loss = 457605.8637822398\n",
      "Iteration 345: Loss = 457605.8147704761\n",
      "Iteration 346: Loss = 457605.76646714995\n",
      "Iteration 347: Loss = 457605.7188619877\n",
      "Iteration 348: Loss = 457605.67194490903\n",
      "Iteration 349: Loss = 457605.6257060183\n",
      "Iteration 350: Loss = 457605.58013546956\n",
      "Iteration 351: Loss = 457605.5352236358\n",
      "Iteration 352: Loss = 457605.4909609987\n",
      "Iteration 353: Loss = 457605.44733818\n",
      "Iteration 354: Loss = 457605.40434591816\n",
      "Iteration 355: Loss = 457605.3619751266\n",
      "Iteration 356: Loss = 457605.32021681214\n",
      "Iteration 357: Loss = 457605.27906212164\n",
      "Iteration 358: Loss = 457605.238502369\n",
      "Iteration 359: Loss = 457605.19852891617\n",
      "Iteration 360: Loss = 457605.1591333094\n",
      "Iteration 361: Loss = 457605.1203072236\n",
      "Iteration 362: Loss = 457605.082042397\n",
      "Iteration 363: Loss = 457605.04433073866\n",
      "Iteration 364: Loss = 457605.0071642311\n",
      "Iteration 365: Loss = 457604.9705350237\n",
      "Iteration 366: Loss = 457604.9344353509\n",
      "Iteration 367: Loss = 457604.898857544\n",
      "Iteration 368: Loss = 457604.8637940865\n",
      "Iteration 369: Loss = 457604.82923752384\n",
      "Iteration 370: Loss = 457604.79518054635\n",
      "Iteration 371: Loss = 457604.76161594\n",
      "Iteration 372: Loss = 457604.7285365744\n",
      "Iteration 373: Loss = 457604.69593544444\n",
      "Iteration 374: Loss = 457604.6638056195\n",
      "Iteration 375: Loss = 457604.63214031875\n",
      "Iteration 376: Loss = 457604.6009328227\n",
      "Iteration 377: Loss = 457604.57017650874\n",
      "Iteration 378: Loss = 457604.5398648648\n",
      "Iteration 379: Loss = 457604.5099914613\n",
      "Iteration 380: Loss = 457604.48054994765\n",
      "Iteration 381: Loss = 457604.4515341186\n",
      "Iteration 382: Loss = 457604.4229377844\n",
      "Iteration 383: Loss = 457604.39475490584\n",
      "Iteration 384: Loss = 457604.366979496\n",
      "Iteration 385: Loss = 457604.3396056853\n",
      "Iteration 386: Loss = 457604.3126276414\n",
      "Iteration 387: Loss = 457604.28603965027\n",
      "Iteration 388: Loss = 457604.2598361075\n",
      "Iteration 389: Loss = 457604.2340114123\n",
      "Iteration 390: Loss = 457604.20856012177\n",
      "Iteration 391: Loss = 457604.18347681384\n",
      "Iteration 392: Loss = 457604.15875619056\n",
      "Iteration 393: Loss = 457604.1343929866\n",
      "Iteration 394: Loss = 457604.1103820621\n",
      "Iteration 395: Loss = 457604.0867183113\n",
      "Iteration 396: Loss = 457604.06339672796\n",
      "Iteration 397: Loss = 457604.0404123467\n",
      "Iteration 398: Loss = 457604.0177602925\n",
      "Iteration 399: Loss = 457603.9954357918\n",
      "Iteration 400: Loss = 457603.9734340845\n",
      "Iteration 401: Loss = 457603.9517505136\n",
      "Iteration 402: Loss = 457603.93038048415\n",
      "Iteration 403: Loss = 457603.9093194397\n",
      "Iteration 404: Loss = 457603.8885629569\n",
      "Iteration 405: Loss = 457603.86810658785\n",
      "Iteration 406: Loss = 457603.847946012\n",
      "Iteration 407: Loss = 457603.82807697856\n",
      "Iteration 408: Loss = 457603.8084952254\n",
      "Iteration 409: Loss = 457603.7891966372\n",
      "Iteration 410: Loss = 457603.7701771066\n",
      "Iteration 411: Loss = 457603.7514326178\n",
      "Iteration 412: Loss = 457603.7329591606\n",
      "Iteration 413: Loss = 457603.71475283155\n",
      "Iteration 414: Loss = 457603.6968097896\n",
      "Iteration 415: Loss = 457603.67912619945\n",
      "Iteration 416: Loss = 457603.6616983276\n",
      "Iteration 417: Loss = 457603.64452245954\n",
      "Iteration 418: Loss = 457603.62759498396\n",
      "Iteration 419: Loss = 457603.61091227294\n",
      "Iteration 420: Loss = 457603.5944708153\n",
      "Iteration 421: Loss = 457603.5782671156\n",
      "Iteration 422: Loss = 457603.5622977216\n",
      "Iteration 423: Loss = 457603.5465592662\n",
      "Iteration 424: Loss = 457603.5310483947\n",
      "Iteration 425: Loss = 457603.51576184516\n",
      "Iteration 426: Loss = 457603.5006963413\n",
      "Iteration 427: Loss = 457603.4858486978\n",
      "Iteration 428: Loss = 457603.4712157684\n",
      "Iteration 429: Loss = 457603.4567944553\n",
      "Iteration 430: Loss = 457603.4425816815\n",
      "Iteration 431: Loss = 457603.42857445654\n",
      "Iteration 432: Loss = 457603.4147697769\n",
      "Iteration 433: Loss = 457603.40116474446\n",
      "Iteration 434: Loss = 457603.38775644475\n",
      "Iteration 435: Loss = 457603.374542064\n",
      "Iteration 436: Loss = 457603.36151878303\n",
      "Iteration 437: Loss = 457603.3486838315\n",
      "Iteration 438: Loss = 457603.3360344923\n",
      "Iteration 439: Loss = 457603.3235680765\n",
      "Iteration 440: Loss = 457603.31128196325\n",
      "Iteration 441: Loss = 457603.2991735141\n",
      "Iteration 442: Loss = 457603.2872401811\n",
      "Iteration 443: Loss = 457603.27547942434\n",
      "Iteration 444: Loss = 457603.2638887436\n",
      "Iteration 445: Loss = 457603.2524656928\n",
      "Iteration 446: Loss = 457603.2412078321\n",
      "Iteration 447: Loss = 457603.23011278594\n",
      "Iteration 448: Loss = 457603.2191782039\n",
      "Iteration 449: Loss = 457603.20840174734\n",
      "Iteration 450: Loss = 457603.1977811393\n",
      "Iteration 451: Loss = 457603.18731413333\n",
      "Iteration 452: Loss = 457603.1769985084\n",
      "Iteration 453: Loss = 457603.16683206306\n",
      "Iteration 454: Loss = 457603.1568126479\n",
      "Iteration 455: Loss = 457603.14693813224\n",
      "Iteration 456: Loss = 457603.13720641413\n",
      "Iteration 457: Loss = 457603.12761546136\n",
      "Iteration 458: Loss = 457603.1181632038\n",
      "Iteration 459: Loss = 457603.10884764884\n",
      "Iteration 460: Loss = 457603.0996668262\n",
      "Iteration 461: Loss = 457603.0906187801\n",
      "Iteration 462: Loss = 457603.081701578\n",
      "Iteration 463: Loss = 457603.0729133534\n",
      "Iteration 464: Loss = 457603.06425221445\n",
      "Iteration 465: Loss = 457603.0557163544\n",
      "Iteration 466: Loss = 457603.04730392824\n",
      "Iteration 467: Loss = 457603.0390131907\n",
      "Iteration 468: Loss = 457603.03084234043\n",
      "Iteration 469: Loss = 457603.02278966643\n",
      "Iteration 470: Loss = 457603.014853456\n",
      "Iteration 471: Loss = 457603.0070320289\n",
      "Iteration 472: Loss = 457602.9993237094\n",
      "Iteration 473: Loss = 457602.9917268811\n",
      "Iteration 474: Loss = 457602.98423992516\n",
      "Iteration 475: Loss = 457602.97686126217\n",
      "Iteration 476: Loss = 457602.9695892973\n",
      "Iteration 477: Loss = 457602.9624225223\n",
      "Iteration 478: Loss = 457602.9553593809\n",
      "Iteration 479: Loss = 457602.9483984075\n",
      "Iteration 480: Loss = 457602.941538113\n",
      "Iteration 481: Loss = 457602.9347770179\n",
      "Iteration 482: Loss = 457602.92811372754\n",
      "Iteration 483: Loss = 457602.9215467994\n",
      "Iteration 484: Loss = 457602.9150748602\n",
      "Iteration 485: Loss = 457602.90869652544\n",
      "Iteration 486: Loss = 457602.90241043316\n",
      "Iteration 487: Loss = 457602.89621526375\n",
      "Iteration 488: Loss = 457602.8901096897\n",
      "Iteration 489: Loss = 457602.8840924302\n",
      "Iteration 490: Loss = 457602.8781621854\n",
      "Iteration 491: Loss = 457602.872317724\n",
      "Iteration 492: Loss = 457602.86655779055\n",
      "Iteration 493: Loss = 457602.86088115803\n",
      "Iteration 494: Loss = 457602.85528663197\n",
      "Iteration 495: Loss = 457602.84977301286\n",
      "Iteration 496: Loss = 457602.84433915565\n",
      "Iteration 497: Loss = 457602.8389838745\n",
      "Iteration 498: Loss = 457602.83370606054\n",
      "Iteration 499: Loss = 457602.8285045749\n",
      "Iteration 500: Loss = 457602.82337832503\n",
      "Iteration 501: Loss = 457602.8183262141\n",
      "Iteration 502: Loss = 457602.8133471781\n",
      "Iteration 503: Loss = 457602.80844015564\n",
      "Iteration 504: Loss = 457602.8036041092\n",
      "Iteration 505: Loss = 457602.79883799935\n",
      "Iteration 506: Loss = 457602.79414083104\n",
      "Iteration 507: Loss = 457602.7895116008\n",
      "Iteration 508: Loss = 457602.7849493268\n",
      "Iteration 509: Loss = 457602.78045303025\n",
      "Iteration 510: Loss = 457602.7760217651\n",
      "Iteration 511: Loss = 457602.7716546004\n",
      "Iteration 512: Loss = 457602.76735060534\n",
      "Iteration 513: Loss = 457602.76310884877\n",
      "Iteration 514: Loss = 457602.7589284618\n",
      "Iteration 515: Loss = 457602.754808525\n",
      "Iteration 516: Loss = 457602.7507481795\n",
      "Iteration 517: Loss = 457602.7467465647\n",
      "Iteration 518: Loss = 457602.7428028263\n",
      "Iteration 519: Loss = 457602.7389161208\n",
      "Iteration 520: Loss = 457602.73508564825\n",
      "Iteration 521: Loss = 457602.7313105715\n",
      "Iteration 522: Loss = 457602.72759009287\n",
      "Iteration 523: Loss = 457602.7239234311\n",
      "Iteration 524: Loss = 457602.7203097999\n",
      "Iteration 525: Loss = 457602.7167484443\n",
      "Iteration 526: Loss = 457602.7132385864\n",
      "Iteration 527: Loss = 457602.7097794981\n",
      "Iteration 528: Loss = 457602.7063704564\n",
      "Iteration 529: Loss = 457602.7030106998\n",
      "Iteration 530: Loss = 457602.6996995531\n",
      "Iteration 531: Loss = 457602.69643629174\n",
      "Iteration 532: Loss = 457602.6932202377\n",
      "Iteration 533: Loss = 457602.69005069917\n",
      "Iteration 534: Loss = 457602.68692699505\n",
      "Iteration 535: Loss = 457602.6838484844\n",
      "Iteration 536: Loss = 457602.680814493\n",
      "Iteration 537: Loss = 457602.67782438756\n",
      "Iteration 538: Loss = 457602.6748775318\n",
      "Iteration 539: Loss = 457602.6719732911\n",
      "Iteration 540: Loss = 457602.6691110621\n",
      "Iteration 541: Loss = 457602.6662902403\n",
      "Iteration 542: Loss = 457602.6635102129\n",
      "Iteration 543: Loss = 457602.6607704086\n",
      "Iteration 544: Loss = 457602.65807021095\n",
      "Iteration 545: Loss = 457602.6554090826\n",
      "Iteration 546: Loss = 457602.6527864427\n",
      "Iteration 547: Loss = 457602.65020173334\n",
      "Iteration 548: Loss = 457602.6476544072\n",
      "Iteration 549: Loss = 457602.6451439366\n",
      "Iteration 550: Loss = 457602.64266977063\n",
      "Iteration 551: Loss = 457602.64023140055\n",
      "Iteration 552: Loss = 457602.63782828656\n",
      "Iteration 553: Loss = 457602.6354599408\n",
      "Iteration 554: Loss = 457602.63312585413\n",
      "Iteration 555: Loss = 457602.6308255118\n",
      "Iteration 556: Loss = 457602.6285584642\n",
      "Iteration 557: Loss = 457602.626324192\n",
      "Iteration 558: Loss = 457602.6241222372\n",
      "Iteration 559: Loss = 457602.62195213675\n",
      "Iteration 560: Loss = 457602.6198134283\n",
      "Iteration 561: Loss = 457602.61770564405\n",
      "Iteration 562: Loss = 457602.6156283566\n",
      "Iteration 563: Loss = 457602.6135811032\n",
      "Iteration 564: Loss = 457602.6115634648\n",
      "Iteration 565: Loss = 457602.6095750347\n",
      "Iteration 566: Loss = 457602.6076153346\n",
      "Iteration 567: Loss = 457602.6056840066\n",
      "Iteration 568: Loss = 457602.60378059634\n",
      "Iteration 569: Loss = 457602.60190472077\n",
      "Iteration 570: Loss = 457602.60005597456\n",
      "Iteration 571: Loss = 457602.59823396953\n",
      "Iteration 572: Loss = 457602.59643832466\n",
      "Iteration 573: Loss = 457602.594668651\n",
      "Iteration 574: Loss = 457602.59292457387\n",
      "Iteration 575: Loss = 457602.59120574006\n",
      "Iteration 576: Loss = 457602.5895117433\n",
      "Iteration 577: Loss = 457602.58784225635\n",
      "Iteration 578: Loss = 457602.58619692246\n",
      "Iteration 579: Loss = 457602.58457536966\n",
      "Iteration 580: Loss = 457602.5829773026\n",
      "Iteration 581: Loss = 457602.58140231174\n",
      "Iteration 582: Loss = 457602.57985014905\n",
      "Iteration 583: Loss = 457602.57832040946\n",
      "Iteration 584: Loss = 457602.5768127964\n",
      "Iteration 585: Loss = 457602.57532699645\n",
      "Iteration 586: Loss = 457602.57386268565\n",
      "Iteration 587: Loss = 457602.5724195507\n",
      "Iteration 588: Loss = 457602.5709973034\n",
      "Iteration 589: Loss = 457602.5695956158\n",
      "Iteration 590: Loss = 457602.56821419817\n",
      "Iteration 591: Loss = 457602.56685277796\n",
      "Iteration 592: Loss = 457602.5655110316\n",
      "Iteration 593: Loss = 457602.5641887081\n",
      "Iteration 594: Loss = 457602.5628855079\n",
      "Iteration 595: Loss = 457602.56160116056\n",
      "Iteration 596: Loss = 457602.56033538235\n",
      "Iteration 597: Loss = 457602.5590879249\n",
      "Iteration 598: Loss = 457602.5578584876\n",
      "Iteration 599: Loss = 457602.55664686544\n",
      "Iteration 600: Loss = 457602.5554527356\n",
      "Iteration 601: Loss = 457602.5542759025\n",
      "Iteration 602: Loss = 457602.55311608163\n",
      "Iteration 603: Loss = 457602.55197305063\n",
      "Iteration 604: Loss = 457602.5508465442\n",
      "Iteration 605: Loss = 457602.54973633337\n",
      "Iteration 606: Loss = 457602.5486421733\n",
      "Iteration 607: Loss = 457602.5475638339\n",
      "Iteration 608: Loss = 457602.5465011119\n",
      "Iteration 609: Loss = 457602.5454537577\n",
      "Iteration 610: Loss = 457602.54442154325\n",
      "Iteration 611: Loss = 457602.543404282\n",
      "Iteration 612: Loss = 457602.54240169487\n",
      "Iteration 613: Loss = 457602.54141364776\n",
      "Iteration 1: Loss = 496784.3579389605\n",
      "Iteration 2: Loss = 466397.9594196245\n",
      "Iteration 3: Loss = 465089.46480885736\n",
      "Iteration 4: Loss = 464961.30286159064\n",
      "Iteration 5: Loss = 464907.36491695524\n",
      "Iteration 6: Loss = 464860.0919401405\n",
      "Iteration 7: Loss = 464814.9903868944\n",
      "Iteration 8: Loss = 464771.695236772\n",
      "Iteration 9: Loss = 464730.1191446842\n",
      "Iteration 10: Loss = 464690.19566184457\n",
      "Iteration 11: Loss = 464651.86186105444\n",
      "Iteration 12: Loss = 464615.0570821995\n",
      "Iteration 13: Loss = 464579.7227852363\n",
      "Iteration 14: Loss = 464545.8024843822\n",
      "Iteration 15: Loss = 464513.2416897785\n",
      "Iteration 16: Loss = 464481.98785051977\n",
      "Iteration 17: Loss = 464451.99029811\n",
      "Iteration 18: Loss = 464423.20019084244\n",
      "Iteration 19: Loss = 464395.57045865594\n",
      "Iteration 20: Loss = 464369.0557488049\n",
      "Iteration 21: Loss = 464343.6123724424\n",
      "Iteration 22: Loss = 464319.1982520707\n",
      "Iteration 23: Loss = 464295.77286990767\n",
      "Iteration 24: Loss = 464273.2972174011\n",
      "Iteration 25: Loss = 464251.73374565056\n",
      "Iteration 26: Loss = 464231.0463169286\n",
      "Iteration 27: Loss = 464211.2001574278\n",
      "Iteration 28: Loss = 464192.1618110541\n",
      "Iteration 29: Loss = 464173.8990942923\n",
      "Iteration 30: Loss = 464156.3810523598\n",
      "Iteration 31: Loss = 464139.5779164174\n",
      "Iteration 32: Loss = 464123.46106199356\n",
      "Iteration 33: Loss = 464108.00296848576\n",
      "Iteration 34: Loss = 464093.1771799192\n",
      "Iteration 35: Loss = 464078.9582667462\n",
      "Iteration 36: Loss = 464065.3217888162\n",
      "Iteration 37: Loss = 464052.24425948755\n",
      "Iteration 38: Loss = 464039.7031107655\n",
      "Iteration 39: Loss = 464027.676659591\n",
      "Iteration 40: Loss = 464016.14407518914\n",
      "Iteration 41: Loss = 464005.08534738724\n",
      "Iteration 42: Loss = 463994.4812560605\n",
      "Iteration 43: Loss = 463984.31334154494\n",
      "Iteration 44: Loss = 463974.56387601595\n",
      "Iteration 45: Loss = 463965.215835804\n",
      "Iteration 46: Loss = 463956.2528747502\n",
      "Iteration 47: Loss = 463947.65929833264\n",
      "Iteration 48: Loss = 463939.4200388596\n",
      "Iteration 49: Loss = 463931.52063128306\n",
      "Iteration 50: Loss = 463923.9471901944\n",
      "Iteration 51: Loss = 463916.6863872604\n",
      "Iteration 52: Loss = 463909.7254298167\n",
      "Iteration 53: Loss = 463903.05203991197\n",
      "Iteration 54: Loss = 463896.6544343893\n",
      "Iteration 55: Loss = 463890.52130550914\n",
      "Iteration 56: Loss = 463884.641802315\n",
      "Iteration 57: Loss = 463879.005512738\n",
      "Iteration 58: Loss = 463873.6024463534\n",
      "Iteration 59: Loss = 463868.42301766947\n",
      "Iteration 60: Loss = 463863.45803021593\n",
      "Iteration 61: Loss = 463858.6986611198\n",
      "Iteration 62: Loss = 463854.13644623524\n",
      "Iteration 63: Loss = 463849.76326592953\n",
      "Iteration 64: Loss = 463845.57133133605\n",
      "Iteration 65: Loss = 463841.5531711848\n",
      "Iteration 66: Loss = 463837.7016190733\n",
      "Iteration 67: Loss = 463834.0098012948\n",
      "Iteration 68: Loss = 463830.4711251111\n",
      "Iteration 69: Loss = 463827.0792674103\n",
      "Iteration 70: Loss = 463823.8281639461\n",
      "Iteration 71: Loss = 463820.71199884557\n",
      "Iteration 72: Loss = 463817.7251946209\n",
      "Iteration 73: Loss = 463814.8624025818\n",
      "Iteration 74: Loss = 463812.11849351367\n",
      "Iteration 75: Loss = 463809.4885488473\n",
      "Iteration 76: Loss = 463806.9678521343\n",
      "Iteration 77: Loss = 463804.5518808085\n",
      "Iteration 78: Loss = 463802.23629831575\n",
      "Iteration 79: Loss = 463800.0169466197\n",
      "Iteration 80: Loss = 463797.8898388183\n",
      "Iteration 81: Loss = 463795.85115234606\n",
      "Iteration 82: Loss = 463793.8972220664\n",
      "Iteration 83: Loss = 463792.02453406615\n",
      "Iteration 84: Loss = 463790.22971930326\n",
      "Iteration 85: Loss = 463788.5095477701\n",
      "Iteration 86: Loss = 463786.86092285765\n",
      "Iteration 87: Loss = 463785.28087575926\n",
      "Iteration 88: Loss = 463783.76656039077\n",
      "Iteration 89: Loss = 463782.3152482246\n",
      "Iteration 90: Loss = 463780.92432359286\n",
      "Iteration 91: Loss = 463779.5912789375\n",
      "Iteration 92: Loss = 463778.31371047243\n",
      "Iteration 93: Loss = 463777.08931389294\n",
      "Iteration 94: Loss = 463775.91588022246\n",
      "Iteration 95: Loss = 463774.79129198694\n",
      "Iteration 96: Loss = 463773.7135193838\n",
      "Iteration 97: Loss = 463772.6806167072\n",
      "Iteration 98: Loss = 463771.69071886595\n",
      "Iteration 99: Loss = 463770.7420380885\n",
      "Iteration 100: Loss = 463769.832860706\n",
      "Iteration 101: Loss = 463768.9615440767\n",
      "Iteration 102: Loss = 463768.1265137299\n",
      "Iteration 103: Loss = 463767.32626048935\n",
      "Iteration 104: Loss = 463766.5593377842\n",
      "Iteration 105: Loss = 463765.82435909536\n",
      "Iteration 106: Loss = 463765.11999544234\n",
      "Iteration 107: Loss = 463764.4449730228\n",
      "Iteration 108: Loss = 463763.7980709016\n",
      "Iteration 109: Loss = 463763.17811886274\n",
      "Iteration 110: Loss = 463762.58399531216\n",
      "Iteration 111: Loss = 463762.0146252189\n",
      "Iteration 112: Loss = 463761.4689782166\n",
      "Iteration 113: Loss = 463760.94606678054\n",
      "Iteration 114: Loss = 463760.4449444048\n",
      "Iteration 115: Loss = 463759.9647039153\n",
      "Iteration 116: Loss = 463759.5044758838\n",
      "Iteration 117: Loss = 463759.0634270017\n",
      "Iteration 118: Loss = 463758.64075861\n",
      "Iteration 119: Loss = 463758.2357052599\n",
      "Iteration 120: Loss = 463757.84753336094\n",
      "Iteration 121: Loss = 463757.4755397858\n",
      "Iteration 122: Loss = 463757.11905072076\n",
      "Iteration 123: Loss = 463756.7774203382\n",
      "Iteration 124: Loss = 463756.45002968895\n",
      "Iteration 125: Loss = 463756.136285567\n",
      "Iteration 126: Loss = 463755.835619492\n",
      "Iteration 127: Loss = 463755.54748660274\n",
      "Iteration 128: Loss = 463755.2713647187\n",
      "Iteration 129: Loss = 463755.00675341964\n",
      "Iteration 130: Loss = 463754.75317310274\n",
      "Iteration 131: Loss = 463754.5101641142\n",
      "Iteration 132: Loss = 463754.27728594764\n",
      "Iteration 133: Loss = 463754.0541164522\n",
      "Iteration 134: Loss = 463753.84025104303\n",
      "Iteration 135: Loss = 463753.6353019837\n",
      "Iteration 136: Loss = 463753.4388976706\n",
      "Iteration 137: Loss = 463753.250681997\n",
      "Iteration 138: Loss = 463753.0703136715\n",
      "Iteration 139: Loss = 463752.8974656272\n",
      "Iteration 140: Loss = 463752.7318244367\n",
      "Iteration 141: Loss = 463752.57308967086\n",
      "Iteration 142: Loss = 463752.4209734943\n",
      "Iteration 143: Loss = 463752.2751999977\n",
      "Iteration 144: Loss = 463752.1355047984\n",
      "Iteration 145: Loss = 463752.0016345109\n",
      "Iteration 146: Loss = 463751.87334633153\n",
      "Iteration 147: Loss = 463751.75040756195\n",
      "Iteration 148: Loss = 463751.6325951733\n",
      "Iteration 149: Loss = 463751.5196954689\n",
      "Iteration 150: Loss = 463751.4115036339\n",
      "Iteration 151: Loss = 463751.30782340735\n",
      "Iteration 152: Loss = 463751.2084666728\n",
      "Iteration 153: Loss = 463751.11325321486\n",
      "Iteration 154: Loss = 463751.02201025095\n",
      "Iteration 155: Loss = 463750.9345722552\n",
      "Iteration 156: Loss = 463750.85078057705\n",
      "Iteration 157: Loss = 463750.77048320277\n",
      "Iteration 158: Loss = 463750.693534417\n",
      "Iteration 159: Loss = 463750.6197946163\n",
      "Iteration 160: Loss = 463750.54912998877\n",
      "Iteration 161: Loss = 463750.48141231085\n",
      "Iteration 162: Loss = 463750.4165186963\n",
      "Iteration 163: Loss = 463750.3543313937\n",
      "Iteration 164: Loss = 463750.2947375581\n",
      "Iteration 165: Loss = 463750.2376290508\n",
      "Iteration 166: Loss = 463750.1829022242\n",
      "Iteration 167: Loss = 463750.1304577742\n",
      "Iteration 168: Loss = 463750.0802005108\n",
      "Iteration 169: Loss = 463750.0320392461\n",
      "Iteration 170: Loss = 463749.98588655674\n",
      "Iteration 171: Loss = 463749.9416586985\n",
      "Iteration 172: Loss = 463749.8992754058\n",
      "Iteration 173: Loss = 463749.8586597429\n",
      "Iteration 174: Loss = 463749.8197380027\n",
      "Iteration 175: Loss = 463749.7824395444\n",
      "Iteration 176: Loss = 463749.7466966752\n",
      "Iteration 177: Loss = 463749.7124445199\n",
      "Iteration 178: Loss = 463749.6796209088\n",
      "Iteration 179: Loss = 463749.6481662769\n",
      "Iteration 180: Loss = 463749.61802352226\n",
      "Iteration 181: Loss = 463749.5891379275\n",
      "Iteration 182: Loss = 463749.56145708653\n",
      "Iteration 183: Loss = 463749.5349307328\n",
      "Iteration 184: Loss = 463749.50951073284\n",
      "Iteration 185: Loss = 463749.48515093996\n",
      "Iteration 186: Loss = 463749.4618071552\n",
      "Iteration 187: Loss = 463749.43943695875\n",
      "Iteration 188: Loss = 463749.4179998083\n",
      "Iteration 189: Loss = 463749.39745674684\n",
      "Iteration 190: Loss = 463749.37777050666\n",
      "Iteration 191: Loss = 463749.3589053403\n",
      "Iteration 192: Loss = 463749.34082701174\n",
      "Iteration 193: Loss = 463749.323502692\n",
      "Iteration 194: Loss = 463749.30690095853\n",
      "Iteration 195: Loss = 463749.2909916581\n",
      "Iteration 196: Loss = 463749.2757459187\n",
      "Iteration 197: Loss = 463749.2611360434\n",
      "Iteration 198: Loss = 463749.2471355341\n",
      "Iteration 199: Loss = 463749.2337189948\n",
      "Iteration 200: Loss = 463749.2208620135\n",
      "Iteration 201: Loss = 463749.208541295\n",
      "Iteration 202: Loss = 463749.19673447026\n",
      "Iteration 203: Loss = 463749.18542008527\n",
      "Iteration 204: Loss = 463749.1745776253\n",
      "Iteration 205: Loss = 463749.16418739036\n",
      "Iteration 206: Loss = 463749.1542305326\n",
      "Iteration 207: Loss = 463749.14468897507\n",
      "Iteration 208: Loss = 463749.1355453715\n",
      "Iteration 209: Loss = 463749.1267831436\n",
      "Iteration 210: Loss = 463749.1183863914\n",
      "Iteration 211: Loss = 463749.1103398687\n",
      "Iteration 212: Loss = 463749.1026289555\n",
      "Iteration 213: Loss = 463749.09523966536\n",
      "Iteration 214: Loss = 463749.0881585933\n",
      "Iteration 215: Loss = 463749.0813728536\n",
      "Iteration 216: Loss = 463749.0748701424\n",
      "Iteration 217: Loss = 463749.06863866845\n",
      "Iteration 218: Loss = 463749.0626671059\n",
      "Iteration 219: Loss = 463749.0569446081\n",
      "Iteration 220: Loss = 463749.05146081536\n",
      "Iteration 221: Loss = 463749.0462057291\n",
      "Iteration 222: Loss = 463749.041169843\n",
      "Iteration 223: Loss = 463749.0363440036\n",
      "Iteration 224: Loss = 463749.0317194399\n",
      "Iteration 225: Loss = 463749.02728779195\n",
      "Iteration 226: Loss = 463749.0230409619\n",
      "Iteration 227: Loss = 463749.0189712815\n",
      "Iteration 228: Loss = 463749.0150713509\n",
      "Iteration 229: Loss = 463749.0113340847\n",
      "Iteration 230: Loss = 463749.00775267824\n",
      "Iteration 231: Loss = 463749.00432067376\n",
      "Iteration 232: Loss = 463749.00103181385\n",
      "Iteration 233: Loss = 463748.9978801401\n",
      "Iteration 234: Loss = 463748.9948599184\n",
      "Iteration 235: Loss = 463748.99196566915\n",
      "Iteration 236: Loss = 463748.9891921374\n",
      "Iteration 237: Loss = 463748.9865343123\n",
      "Iteration 238: Loss = 463748.9839873174\n",
      "Iteration 239: Loss = 463748.98154656944\n",
      "Iteration 240: Loss = 463748.9792076118\n",
      "Iteration 241: Loss = 463748.97696623765\n",
      "Iteration 242: Loss = 463748.97481834097\n",
      "Iteration 243: Loss = 463748.9727600264\n",
      "Iteration 244: Loss = 463748.97078757425\n",
      "Iteration 245: Loss = 463748.9688974037\n",
      "Iteration 246: Loss = 463748.9670860514\n",
      "Iteration 247: Loss = 463748.9653502578\n",
      "Iteration 248: Loss = 463748.96368687844\n",
      "Iteration 249: Loss = 463748.96209287015\n",
      "Iteration 250: Loss = 463748.96056534984\n",
      "Iteration 251: Loss = 463748.95910153445\n",
      "Iteration 252: Loss = 463748.95769878034\n",
      "Iteration 253: Loss = 463748.9563545489\n",
      "Iteration 254: Loss = 463748.95506636886\n",
      "Iteration 255: Loss = 463748.95383191935\n",
      "Iteration 256: Loss = 463748.9526489707\n",
      "Iteration 257: Loss = 463748.95151536254\n",
      "Iteration 258: Loss = 463748.95042903256\n",
      "Iteration 259: Loss = 463748.94938801124\n",
      "Iteration 260: Loss = 463748.9483904093\n",
      "Found lamb with lower MSE!!!\n"
     ]
    }
   ],
   "source": [
    "# Coordinate Descent\n",
    "\n",
    "lambdas = [0.5, 1.5, 2]\n",
    "mseDict = defaultdict(float)\n",
    "iterations = 1000\n",
    "tol = 1e-3\n",
    "\n",
    "actual_q8 = [h[2]['hours_transformed'] for h in hoursValid]\n",
    "\n",
    "for lamb in lambdas:\n",
    "    lastObjective = None\n",
    "    bestObjective = None\n",
    "\n",
    "    betaU = {}\n",
    "    betaI = {}\n",
    "    for u in hoursPerUser:\n",
    "        betaU[u] = 0\n",
    "    for g in hoursPerItem:\n",
    "        betaI[g] = 0\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        alpha_ = iterate(lamb)\n",
    "        currObjective = calculate_objective(alpha_, lamb)\n",
    "        print(f'Iteration {iter+1}: Loss = {currObjective}')\n",
    "        if lastObjective and abs(lastObjective-currObjective) < tol:\n",
    "            bestObjective = currObjective\n",
    "            break\n",
    "        lastObjective = currObjective\n",
    "\n",
    "    preds = []\n",
    "    for h in hoursValid:\n",
    "        user, game = h[0], h[1]\n",
    "        pred = alpha_ + betaU[user] + betaI[game]\n",
    "        preds.append(pred)\n",
    "\n",
    "    mse = MSE(np.array(actual_q8), np.array(preds))\n",
    "    mseDict[lamb] = mse\n",
    "    if mse < validMSE:\n",
    "        print(\"Found lamb with lower MSE!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {0.5: 3.013191503236919, 1.5: 3.002692000857554})"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better lambda...\n",
    "\n",
    "mseDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "validMSE = list(mseDict.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b95c8e49-d120-4367-a20f-a39381776979",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (list(mseDict.keys())[1], validMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fe2dcb96-86a0-473e-980b-340435715ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q8'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "90a7cd55-1f58-42a5-8c35-4debf80a3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"HWpredictions_Hours.csv\", 'w')\n",
    "for l in open(\"data/pairs_Hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    # Logic...\n",
    "    alpha = alpha_\n",
    "    bu, bi = betaU[u], betaI[g]\n",
    "    \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(alpha + bu + bi) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.683968396839684,\n",
       " 'Q2': [96249.45000000001, 0.6954195419541954],\n",
       " 'Q3': 0.673067306730673,\n",
       " 'Q4': 0.6867186718671867,\n",
       " 'Q5': 'I confirm that I have uploaded an assignment submission to gradescope',\n",
       " 'Q6': 3.0073728449436676,\n",
       " 'Q7': [5.828516272137527,\n",
       "  -3.0057164456685355,\n",
       "  5.513882931578414,\n",
       "  -2.791543895327124],\n",
       " 'Q8': (1.5, 3.002692000857554)}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c5fe92e3-3ab1-4858-858c-eeb732d964f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused Code Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9676dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainHoursByPair = defaultdict(float)\n",
    "\n",
    "# for h in hoursTrain:\n",
    "#     uid, gid, hours = h[0], h[1], h[2]['hours_transformed']\n",
    "#     trainHoursByPair[(uid, gid)] = hours\n",
    "\n",
    "# lamb = 1\n",
    "\n",
    "# # BetaU\n",
    "# betaUs = defaultdict(float)\n",
    "# for u in gamesPerPlayer:\n",
    "#     numer = []\n",
    "#     for g in gamesPerPlayer[u]:\n",
    "#         diff = trainHoursByPair[(u, g)] - (a + betaI[g])\n",
    "#         numer.append(diff)\n",
    "#     oneBetaU = sum(numer) / (lamb + len(gamesPerPlayer[u]))\n",
    "#     betaUs[u] = oneBetaU\n",
    "\n",
    "# # BetaI\n",
    "# betaIs = defaultdict(float)\n",
    "# for g in playersPerGame:\n",
    "#     numer = []\n",
    "#     for u in playersPerGame[g]:\n",
    "#         diff = trainHoursByPair[(u, g)] - (a + betaU[u])\n",
    "#         numer.append(diff)\n",
    "#     oneBetaI = sum(numer) / (lamb + len(playersPerGame[g]))\n",
    "#     betaIs[g] = oneBetaI\n",
    "\n",
    "# regularizer = lamb*(sum(np.array(list(betaUs.values()))**2) \\\n",
    "#     + sum(np.array(list(betaIs.values()))**2))\n",
    "\n",
    "# optimBetas = None\n",
    "# minError = None\n",
    "\n",
    "# for bu in betaUs.values():\n",
    "#     for bi in betaIs.values():\n",
    "#         se = []\n",
    "#         for pair in trainHoursByPair:\n",
    "#             se.append((a + bu + bi - trainHoursByPair[pair])**2)\n",
    "#         error = sum(se) + regularizer\n",
    "#         if minError is None or minError > error:\n",
    "#             minError = error\n",
    "#             optimBetas = (bu, bi)\n",
    "\n",
    "# u91746794\n",
    "\n",
    "        # def gradient_descent(iter=1000, lr=0.0001, tol=1e-8, lamb=1):\n",
    "\n",
    "        #     # Initializing\n",
    "        #     curr_alpha = alpha\n",
    "        #     # curr_betaU = np.mean(list(betaU.values()))\n",
    "        #     curr_betaU = random.choice(list(betaU.values()))\n",
    "        #     # curr_betaI = np.mean(list(betaI.values()))\n",
    "        #     curr_betaI = random.choice(list(betaI.values()))\n",
    "\n",
    "        #     losses = []\n",
    "        #     prev_loss = None\n",
    "\n",
    "        #     # Estimating optimal params\n",
    "        #     for i in range(iter):\n",
    "        #         # iter_loss = []\n",
    "        #         for u in hoursPerUser:\n",
    "        #             length = len(hoursPerUser[u])\n",
    "        #             actualHours = np.array(hoursPerUser[u])\n",
    "\n",
    "        #             pred = [curr_alpha + curr_betaU + curr_betaI]*length\n",
    "        #             # loss_bu = MSE(actualHours, np.array(pred))\n",
    "        #             # loss.append(loss_bu)\n",
    "\n",
    "        #             betaU_derivative = \\\n",
    "        #                 2*np.sum(np.array(pred)-actualHours) + 2*lamb*curr_betaU\n",
    "                    \n",
    "        #             next_betaU = curr_betaU - (lr * betaU_derivative)\n",
    "\n",
    "        #             if abs(next_betaU - curr_betaU) < tol:\n",
    "        #                 break\n",
    "\n",
    "        #             curr_betaU = next_betaU\n",
    "\n",
    "        #         for g in hoursPerItem:\n",
    "        #             length = len(hoursPerItem[g])\n",
    "        #             actualHours = np.array(hoursPerItem[g])\n",
    "\n",
    "        #             pred = [curr_alpha + curr_betaU + curr_betaI]*length\n",
    "        #             # loss_bi = MSE(actualHours, np.array(pred))\n",
    "        #             # loss.append(loss_bi)\n",
    "\n",
    "        #             betaI_derivative = \\\n",
    "        #                 2*np.sum(np.array(pred)-actualHours) + 2*lamb*curr_betaI\n",
    "                    \n",
    "        #             next_betaI = curr_betaI - (lr * betaI_derivative)\n",
    "\n",
    "        #             if abs(next_betaI - curr_betaI) < tol:\n",
    "        #                 break\n",
    "\n",
    "        #             curr_betaI = next_betaI\n",
    "                \n",
    "        #         # Updating alpha\n",
    "        #         length = len(trainHours)\n",
    "        #         pred = [curr_alpha + curr_betaU + curr_betaI]*length\n",
    "        #         # curr_loss = MSE(np.array(trainHours), np.array(pred))\n",
    "        #         # iter_loss.append(loss)\n",
    "\n",
    "        #         curr_loss = np.sum((np.array(pred)-np.array(trainHours))**2) + \\\n",
    "        #             lamb*(np.sum(np.array(list(betaU.values()))**2) + np.sum(np.array(list(betaI.values()))**2))\n",
    "\n",
    "        #         alpha_derivative = \\\n",
    "        #             2*np.sum(np.array(pred)-np.array(trainHours))\n",
    "\n",
    "        #         next_alpha = curr_alpha - (lr * alpha_derivative)\n",
    "\n",
    "        #         # if np.sum(np.isnan(iter_loss)) > 0:\n",
    "        #         #     nonNullLosses = []\n",
    "        #         #     for l in loss:\n",
    "        #         #         if not np.isnan(l):\n",
    "        #         #             nonNullLosses.append(l)\n",
    "                \n",
    "        #         # curr_loss = np.mean(nonNullLosses)\n",
    "\n",
    "        #         print(f\"Iteration {i+1}: Loss {curr_loss}\")\n",
    "\n",
    "        #         # if prev_loss and abs(prev_loss-curr_loss) <= tol:\n",
    "        #         if abs(next_alpha - curr_alpha) < tol:\n",
    "        #             break\n",
    "\n",
    "        #         prev_loss = curr_loss\n",
    "        #         losses.append(curr_loss)\n",
    "        #         curr_alpha = next_alpha\n",
    "\n",
    "        #         # if i == 55: break\n",
    "\n",
    "        #     return curr_alpha, curr_betaU, curr_betaI\n",
    "\n",
    "\n",
    "# # Coordinate Descent\n",
    "\n",
    "# objLog = []\n",
    "# lastObjective = None\n",
    "# bestObjective = None\n",
    "# iterations = 100\n",
    "# tol = 1e-5\n",
    "# lamb = 1\n",
    "# bu = 0\n",
    "# bi = 0\n",
    "\n",
    "# for iter in range(iterations):\n",
    "#     alpha_cd = calculate_alpha(bu, bi)\n",
    "\n",
    "#     if bestObjective is None:\n",
    "#         bestObjective = calculate_objective(alpha_cd, bu, bi, lamb)\n",
    "#         print(bestObjective)\n",
    "\n",
    "#     for p in hoursPerUser:\n",
    "#         bU_cd = calculate_betaU(p, alpha_cd, bi, lamb)\n",
    "#         newObjective = calculate_objective(alpha_cd, bU_cd, bi, lamb)\n",
    "\n",
    "#         if newObjective < bestObjective:\n",
    "#             bu = bU_cd\n",
    "#             bestObjective = newObjective\n",
    "    \n",
    "#     for g in hoursPerItem:\n",
    "#         bI_cd = calculate_betaI(g, alpha_cd, bu, lamb)\n",
    "#         newObjective = calculate_objective(alpha_cd, bu, bI_cd, lamb)\n",
    "\n",
    "#         if newObjective < bestObjective:\n",
    "#             bi = bI_cd\n",
    "#             bestObjective = newObjective\n",
    "\n",
    "#     # if lastObjective and abs(lastObjective-bestObjective) < tol:\n",
    "#     #     break\n",
    "\n",
    "#     lastObjective = bestObjective\n",
    "#     objLog.append(lastObjective)\n",
    "\n",
    "#     print(f'Iteration {iter+1}: Objective = {bestObjective}')\n",
    "\n",
    "# userEncoder = {}\n",
    "\n",
    "# counter = 0\n",
    "# for u in hoursPerUser:\n",
    "#     encoder = [0]*(len(hoursPerUser)-1)\n",
    "#     if counter > 0:\n",
    "#         encoder[counter-1] = 1\n",
    "#     userEncoder[u] = encoder\n",
    "#     counter += 1\n",
    "\n",
    "# itemEncoder = {}\n",
    "\n",
    "# counter = 0\n",
    "# for g in hoursPerItem:\n",
    "#     encoder = [0]*(len(hoursPerItem)-1)\n",
    "#     if counter > 0:\n",
    "#         encoder[counter-1] = 1\n",
    "#     itemEncoder[g] = encoder\n",
    "#     counter += 1\n",
    "\n",
    "# X = [[1]+userEncoder[u[0]]+itemEncoder[u[1]] for u in trainHoursByPair]\n",
    "# y = trainHours\n",
    "# mdl = linear_model.SGDRegressor(fit_intercept=False, alpha=1)\n",
    "# mdl.fit(X, y)\n",
    "# preds_sklearn = mdl.predict(X[:5])\n",
    "# mse_sklearn = MSE(np.array(trainHours[:5]), preds_sklearn)\n",
    "\n",
    "# x2 = [[1, 1, 1]]*len(trainHours)\n",
    "# mdl2 = linear_model.SGDRegressor(fit_intercept=False)\n",
    "# mdl2.fit(x2, y)\n",
    "# preds2 = mdl2.predict(x2)\n",
    "# mse_sklearn2 = MSE(np.array(trainHours), preds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
